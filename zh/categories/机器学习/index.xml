<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 | 杜利强</title>
    <link>https://keris.github.io/zh/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
      <atom:link href="https://keris.github.io/zh/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <description>机器学习</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>zh-Hans</language><lastBuildDate>Wed, 11 Mar 2020 19:20:11 +0800</lastBuildDate>
    <image>
      <url>https://keris.github.io/images/icon_hu0fc6cda6d9ee8f97aed5fca718c40606_62996_512x512_fill_lanczos_center_2.png</url>
      <title>机器学习</title>
      <link>https://keris.github.io/zh/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    </image>
    
    <item>
      <title>评估指标</title>
      <link>https://keris.github.io/zh/post/metrics/</link>
      <pubDate>Wed, 11 Mar 2020 19:20:11 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/metrics/</guid>
      <description>&lt;h2&gt;目录&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#混淆矩阵&#34;&gt;混淆矩阵&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#accuracy&#34;&gt;Accuracy&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#precision&#34;&gt;Precision&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#recall&#34;&gt;Recall&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#f1-score&#34;&gt;F1 score&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#f_beta-score&#34;&gt;$F_\beta$ score&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#roc-curve&#34;&gt;ROC Curve&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#auc&#34;&gt;AUC&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#回归指标&#34;&gt;回归指标&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;混淆矩阵&#34;&gt;混淆矩阵&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;guessed positive&lt;/th&gt;
&lt;th&gt;guessed negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;positive&lt;/td&gt;
&lt;td&gt;true positives, #TP&lt;/td&gt;
&lt;td&gt;false negatives, #FN&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;td&gt;false positives, #FP&lt;/td&gt;
&lt;td&gt;true negatives, #TN&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;假设某个模型在一组样本上的表现如下：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;model-performance.png&#34; &gt;
&lt;img data-src=&#34;model-performance.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;其中，蓝色的点为正例（positives)，橙色的点为负例（negatives），则混淆矩阵如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;guessed positive&lt;/th&gt;
&lt;th&gt;guessed negative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;positive&lt;/td&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;negative&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;accuracy&#34;&gt;Accuracy&lt;/h2&gt;
&lt;p&gt;Accuracy，也就是准确率，表示样本中分类正确所占的比例。&lt;/p&gt;
&lt;p&gt;$$\text{Accuracy} = \frac{\text{#TP} + \text{#TN}}{\text{#TP} + \text{#FP} + \text{#TN} + \text{#FN}}$$&lt;/p&gt;
&lt;p&gt;其中:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;#TP&lt;/code&gt;表示&lt;code&gt;true positive&lt;/code&gt;的数目&lt;/li&gt;
&lt;li&gt;&lt;code&gt;#FP&lt;/code&gt;表示&lt;code&gt;false positive&lt;/code&gt;的数目&lt;/li&gt;
&lt;li&gt;&lt;code&gt;#TN&lt;/code&gt;表示&lt;code&gt;true negative&lt;/code&gt;的数目&lt;/li&gt;
&lt;li&gt;&lt;code&gt;#FN&lt;/code&gt;表示&lt;code&gt;false negative&lt;/code&gt;的数目&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于图1中的模型，准确率计算如下：&lt;/p&gt;
&lt;p&gt;$$\text{Accuracy} = \frac{6 + 5}{6 + 1 + 5 + 2} = \frac{11}{14}= 78.57\%$$&lt;/p&gt;
&lt;p&gt;准确率在数据偏斜的情况下将不再适用。比如在下图所示的例子中：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;credit-card-fraud.png&#34; &gt;
&lt;img data-src=&#34;credit-card-fraud.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;模型需要找出良好的交易，但我们的样本中绝大部分都是良好的交易，这导致无论我们的模型如何，它都具有很高的准确率，如图中所示为&lt;code&gt;99.83%&lt;/code&gt;！，此时准确率将不能够评估我们的模型。&lt;/p&gt;
&lt;h2 id=&#34;precision&#34;&gt;Precision&lt;/h2&gt;
&lt;p&gt;Precision即精度，表示在所有预测为正例的样本中有多少是真正例。&lt;/p&gt;
&lt;p&gt;$$\text{Precision} = \frac{\text{#TP}}{\text{#TP} + \text{#FP}}$$&lt;/p&gt;
&lt;p&gt;对于图1中的模型，精度计算如下：&lt;/p&gt;
&lt;p&gt;$$\text{Precision} = \frac{6}{6 + 2} = \frac{6}{8} = 75\%$$&lt;/p&gt;
&lt;h2 id=&#34;recall&#34;&gt;Recall&lt;/h2&gt;
&lt;p&gt;Recall即为召回率，所有的正例样本中真正例所占的比例：&lt;/p&gt;
&lt;p&gt;$$\text{Recall} = \frac{\text{#TP}}{\text{#TP} + \text{#FN}}$$&lt;/p&gt;
&lt;p&gt;对于图1中的模型，召回率计算如下：&lt;/p&gt;
&lt;p&gt;$$\text{Recall} = \frac{6}{6 + 1} = \frac{6}{7} = 85.71\%$$&lt;/p&gt;
&lt;h2 id=&#34;f1-score&#34;&gt;F1 score&lt;/h2&gt;
&lt;p&gt;F1-score定义如下：&lt;/p&gt;
&lt;p&gt;$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$&lt;/p&gt;
&lt;p&gt;可以看到F1-score综合了精度和召回率，为精度和召回率的调和平均：&lt;strong&gt;在召回率不变的条件下，精度越高，F1-score越大；在精度不变的条件下，召回率越高，F1-score越大&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;f_beta-score&#34;&gt;$F_\beta$ score&lt;/h2&gt;
&lt;p&gt;$F_\beta-\text{score}$定义为：&lt;/p&gt;
&lt;p&gt;$$F_\beta = (1 + \beta^2) \times \frac{\text{Precision} \times \text{Recall}}{\beta^2 \times \text{Precision} + \text{Recall}}$$&lt;/p&gt;
&lt;p&gt;可以看出来，当$\beta = 1$时即为F1 score。&lt;/p&gt;
&lt;p&gt;下面我们看下$F_\beta$随$\beta$变化的情况：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;当$\beta = 0$时，通过计算不难得出$F_0 = \text{Precision}$;&lt;/li&gt;
&lt;li&gt;相反，当$\beta$取很大的值时，$F_\beta$将趋近召回率。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;小结：当$\beta$取较小的值时，精度比召回率重要，如$F_{0.5}$ score；当$\beta$取较大值时召回率比精度重要，如$F_2$ score。&lt;/p&gt;
&lt;h2 id=&#34;roc-curve&#34;&gt;ROC Curve&lt;/h2&gt;
&lt;p&gt;ROC Curve的全称为Receiver Operating Characteristic Curve，即受试者工作特性缺陷，它刻画了模型在所有分类阈值下的表现。&lt;/p&gt;
&lt;p&gt;要绘制该曲线，我们需要计算以下两个参数：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;True Positive Rate, TPR&lt;/li&gt;
&lt;li&gt;False Positive Rate, FPR&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TPR，表示所有的正例样本中真正例的占比，不难发现，这其实跟召回率一个概念：&lt;/p&gt;
&lt;p&gt;$$\text{TPR} = \frac{\text{#TP}}{\text{#TP} + \text{#FN}}$$&lt;/p&gt;
&lt;p&gt;FPR，表示所有的负例样本中假正例的占比：&lt;/p&gt;
&lt;p&gt;$$\text{FPR} = \frac{\text{#FP}}{\text{#FP} + \text{#TN}}$$&lt;/p&gt;
&lt;p&gt;在不同的分类阈值下绘制TPR vs. FPR便得到ROC曲线：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;ROCCurve.svg&#34; &gt;
&lt;img data-src=&#34;ROCCurve.svg&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;auc&#34;&gt;AUC&lt;/h2&gt;
&lt;p&gt;AUC的全称为Area Under a ROC Curve，即ROC曲线下的面积。&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;AUC.png&#34; &gt;
&lt;img data-src=&#34;AUC.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;回归指标&#34;&gt;回归指标&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Mean Absolute Error (MAE)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MAE即平均绝对误差，假设我们有一组样本${x_i, y_i}, i = 1, 2, \cdots, n$，模型的输出为$\text{preds} = [p_1, p_2, \cdots, p_n]$，则MAE计算如下：&lt;/p&gt;
&lt;p&gt;$$\text{MAE} = \frac{1}{n}\sum_i^n |y_i - p_i|$$&lt;/p&gt;
&lt;p&gt;借助&lt;code&gt;sklearn&lt;/code&gt;，我们可以很方便地计算MAE：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import LinearRegression
clf = LinearRegression()
preds = clf.fit(X, y)
error = mean_absolute_error(y, preds)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从公式来看，MAE不可导，使得不能采用梯度下降方法进行优化，此时我们可以借助均方误差。&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Mean Squared Error (MSE)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;MSE即均方误差，定义如下：&lt;/p&gt;
&lt;p&gt;$$\text{MSE} = \frac{1}{n}\sum_i^n (y_i - p_i)^2$$&lt;/p&gt;
&lt;p&gt;同样地，MSE通过&lt;code&gt;sklearn&lt;/code&gt;可以很方便地进行计算：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
clf = LinearRegression()
preds = clf.fit(X, y)
error = mean_squared_error(y, preds)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;R2 Score&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;R2分数通过将现有的模型与最简单的可能模型相比得出。&lt;/p&gt;
&lt;p&gt;比如，我们要拟合一堆数据点，那最简单的可能模型是什么？那就是我们取所有值的平均值，然后画一条水平线。此时我们可以计算出这个模型的均方误差大于线性回归模型的均方误差，如下图所示：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;r2.png&#34; &gt;
&lt;img data-src=&#34;r2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;R2分数定义为：&lt;/p&gt;
&lt;p&gt;$$\text{R2} = 1 - \frac{\text{MSE of 当前模型}}{\text{MSE of 最简单的可能模型}}$$&lt;/p&gt;
&lt;p&gt;以上图为例，分子为线性回归模型（右侧）的均方误差，分母为简单模型（左侧）的均方误差。&lt;/p&gt;
&lt;p&gt;由于最简单的可能模型的均方误差大于线性回归模型的均方误差，我们在相比之后得到一个0到1之间的数字，从而R2得分也是一个0到1之间的数，并且有以下结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;R2越接近0，我们的模型越接近最简单的可能模型，此时模型是一个bad model&lt;/li&gt;
&lt;li&gt;相反，R2越接近1，说明我们模型相比最简单的可能模型具有更小的均方误差，此时模型是一个good model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在&lt;code&gt;sklearn&lt;/code&gt;中，我们如下计算R2得分：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import r2_score
&amp;gt;&amp;gt;&amp;gt; y_true = [1, 2, 4]
&amp;gt;&amp;gt;&amp;gt; y_pred = [1.3, 2.5, 3.7]
&amp;gt;&amp;gt;&amp;gt; r2_score(y_true, y_pred)
&amp;gt;&amp;gt;&amp;gt; 0.9078571428571429
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>逻辑回归（Logistic Regression）</title>
      <link>https://keris.github.io/zh/post/logistic-regression/</link>
      <pubDate>Tue, 14 Jan 2020 16:47:22 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/logistic-regression/</guid>
      <description>&lt;h2&gt;目录&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#术语&#34;&gt;术语&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#logistic-function&#34;&gt;Logistic function&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#odds&#34;&gt;Odds&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#logit-function&#34;&gt;Logit function&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#逻辑回归&#34;&gt;逻辑回归&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#代价函数求导&#34;&gt;代价函数求导&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;术语&#34;&gt;术语&lt;/h2&gt;
&lt;p&gt;逻辑回归涉及到以下术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logistic function&lt;/li&gt;
&lt;li&gt;Odds&lt;/li&gt;
&lt;li&gt;Logit&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;logistic-function&#34;&gt;Logistic function&lt;/h3&gt;
&lt;p&gt;逻辑回归中的 &lt;em&gt;Logistic&lt;/em&gt; 正是出于 &lt;em&gt;Logistic function&lt;/em&gt; ，它是一种 &lt;em&gt;sigmoid function&lt;/em&gt; ，其接受任意实值，输出一个0到1之间的值。
&lt;em&gt;标准的&lt;/em&gt; logistic funtion 定义如下：&lt;/p&gt;
&lt;p&gt;$$\sigma(z) = \frac{e^z}{e^z + 1} = \frac{1}{1 + e^{-z}}$$&lt;/p&gt;
&lt;p&gt;如下是它在区间$[-6, 6]$之间的图像：&lt;/p&gt;
&lt;p&gt;&lt;a title=&#34;logistic function curve&#34; href=&#34;https://commons.wikimedia.org/wiki/File:Logistic-curve.svg&#34;&gt;&lt;img width=&#34;512&#34; alt=&#34;Logistic-curve&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/512px-Logistic-curve.svg.png&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在逻辑回归中，我们使用对数几率（log odds），&lt;strong&gt;并假定它是输入特征的线性组合&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;$$z = \ln \frac{p(x)}{1 - p(x)}= \theta_0 + \theta_1 x_1 + \theta_2 x_2 = \theta^T \cdot x$$&lt;/p&gt;
&lt;p&gt;由上式我们可以得到，&lt;/p&gt;
&lt;p&gt;$$p(x) = \sigma(z) = \frac{1}{1 + e^{-\theta^T \cdot x}}$$&lt;/p&gt;
&lt;p&gt;在逻辑回归模型中，这里的 $p(x)$ 为因变量在成功情形下的概率，即 $p(y=1 \mid x)$。&lt;/p&gt;
&lt;h3 id=&#34;odds&#34;&gt;Odds&lt;/h3&gt;
&lt;p&gt;如果 $p$ 表示一个事件发生的概率，那么odds定义为&lt;/p&gt;
&lt;p&gt;$$\text{odds} = \frac{p}{1 - p}$$
也就是说，odds为发生的概率除以不发生的概率，亦可以说为成功的概率除以失败的概率。&lt;/p&gt;
&lt;h3 id=&#34;logit-function&#34;&gt;Logit function&lt;/h3&gt;
&lt;p&gt;Logit为Log odds, logit function 定义为 logistic function的逆，即 $g = \sigma^{-1}$。显而易见，我们有&lt;/p&gt;
&lt;p&gt;$$g(p(x)) = \sigma_{-1}(p(x)) = \text{logit}\,p(x) = \ln(\frac{p(x)}{1 - p(x)}) = \theta^T \cdot x$$&lt;/p&gt;
&lt;h2 id=&#34;逻辑回归&#34;&gt;逻辑回归&lt;/h2&gt;
&lt;p&gt;逻辑回归是一个重要的机器学习算法，其目标是基于给定的数据$x$输出随机变量$y$为0或1的概率。&lt;/p&gt;
&lt;p&gt;考虑由$\theta$参数化的线性模型，&lt;/p&gt;
&lt;p&gt;$$h_\theta(x) = \frac{1}{1 + e^{-\theta^T \cdot x}} = \text{Pr}(y = 1 \mid x;\theta)$$&lt;/p&gt;
&lt;p&gt;从而，$\text{Pr}(y=0 \mid x;\theta) = 1 - h_\theta(x)$。&lt;/p&gt;
&lt;p&gt;因为$y \in \{0, 1 \}$，我们有&lt;/p&gt;
&lt;p&gt;$$\text{Pr}(y \mid x;\theta) = h_\theta(x)^y (1 - h_\theta(x))^{1 - y}
$$&lt;/p&gt;
&lt;p&gt;似然函数为&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} L(\theta \mid x) &amp;amp;= \Pr(Y\mid X;\theta) \\&lt;br&gt;
&amp;amp;= \prod_i \Pr(y^{(i)} \mid x^{(i)};\theta) \\&lt;br&gt;
&amp;amp;= \prod_i h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{aligned}$$&lt;/p&gt;
&lt;p&gt;一般地，我们最大化对数似然函数，&lt;/p&gt;
&lt;p&gt;$$\log L(\theta \mid x) = \sum_{i=1}^{m}\log \Pr(y^{(i)} \mid x^{(i)};\theta)$$&lt;/p&gt;
&lt;p&gt;定义代价函数如下：&lt;/p&gt;
&lt;p&gt;$$J(\theta) = -\frac{1}{m} \log L(\theta \mid x) = -\frac{1}{m} \sum_i^m (y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})(1 - h_\theta(x^{(i)})))$$&lt;/p&gt;
&lt;p&gt;容易看到，我们最大化对数似然函数就是最小化代价函数 $J(\theta)$。在机器学习中，我们使用梯度下降最小化代价函数。&lt;/p&gt;
&lt;h3 id=&#34;代价函数求导&#34;&gt;代价函数求导&lt;/h3&gt;
&lt;p&gt;下面我们对代价函数$J(\theta)$对$\theta$进行求导，我们先考虑在一个样本上的代价函数&lt;/p&gt;
&lt;p&gt;$$J_1(\theta) = -y \log h_\theta(x) - (1 - y)(1 - h_\theta(x))$$&lt;/p&gt;
&lt;p&gt;现在对$J_1(\theta)$对$\theta$进行求导：&lt;/p&gt;
&lt;p&gt;注意到$\frac{d\sigma(z)}{dz} = \sigma(z) (1 - \sigma(z))$, 我们有&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}\frac{\partial}{\partial \theta_j} J_1(\theta) &amp;amp;= -y \frac{1}{h_\theta(x)} h_\theta(x) (1 - h_\theta(x)) x_j - (1 - y) \frac{1}{1 - h_\theta(x)} (-1) h_\theta(x) (1 - h_\theta(x)) x_j \\ &amp;amp;= (h_\theta(x) - y) x_j\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;有了以上结果，我们有&lt;/p&gt;
&lt;p&gt;$$\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m}\sum_i^m [h_\theta(x^{(i)}) - y^{(i)}]x_j^{(i)}$$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

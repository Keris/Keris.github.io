[{"authors":["admin"],"categories":null,"content":"我是杜利强，今年30有余，过了10月便是31，甘肃陇南人氏。在这个地方，我希望记录自己，便于我回头再看的时候发现自己有所成长。各位看官（可能一个也没有）如果有想法可以留言，也可以发邮件给我，甚至直接给我打电话。\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"zh","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://keris.github.io/zh/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/authors/admin/","section":"authors","summary":"我是杜利强，今年30有余，过了10月便是31，甘肃陇南人氏。在这个地方，我希望记录自己，便于我回头再看的时候发现自己有所成长。各位看官（可能一个也没有）如果有想法可以留言，也可以发邮件给我，甚至直接给我打电话。","tags":null,"title":"杜利强","type":"authors"},{"authors":["杜利强"],"categories":["数学"],"content":"棋经有云，“宁失一子，勿失一先”，可见先手的好处所在。为了建立对先手直观的理解，我们来玩一个游戏：\n两个人玩投掷硬币的游戏，先掷出正面的人获胜，那么你是选择先掷还是后掷？\n数学解法 假设硬币是公平的，即出现正面和反面的概率一样，均为1/2。那么，在先手下获胜的情况是第n次掷得正面，前n-1次都为反面，其中n为奇数。所以先手获胜的概率为\n$$p(\\text{win}|\\text{first}) = \\sum_{k=0}^{\\infty}(\\frac{1}{2})^{2k + 1} = \\frac{2}{3}$$\n相反，在后手下获胜的情况是第n次掷得正面，前n-1次均为反面，其中n为偶数。所以，后手获胜的概率为\n$$p(\\text{win}|\\text{second}) = \\sum_{k=1}^{\\infty}(\\frac{1}{2})^{2k} = \\frac{1}{3}$$\n可以发现，先手获胜的概率为2/3，而后手获胜的概率为1/3，这告诉我们在生活中抓住先机，主动出击往往更能取得胜利。\n模拟法 前面我们用数学的方法进行了求解，下面来看看如何用程序来模拟，为此我写了一个Python程序：\n# coding: utf-8 # filename: code.py # Author: Liqiang Du \u0026lt;keris.du@gmail.com\u0026gt; import random import numpy as np def simulate(n_games): \u0026quot;\u0026quot;\u0026quot; Simulate coin throwing game, the game is over when head appears. Args: n_games (int): the number of games. Returns: tuple: the first is the probability of win when first play, and the second the probability of win when second play. \u0026quot;\u0026quot;\u0026quot; win_count = np.array([0, 0]) for i in range(n_games): game_over = False n_tails = 0 while not game_over: x = random.randint(0, 1) # 1 indicates head and 0 tail if x == 1: game_over = True else: n_tails += 1 win_count[n_tails % 2] += 1 return win_count.astype(float) / n_games if __name__ == \u0026quot;__main__\u0026quot;: n_games = 100000 probs = simulate(n_games) print(probs)  程序的核心部分在于simulate，每次游戏，出现正面游戏就结束，同时记录反面出现的次数，如果该次数为偶数，则先手为赢，否则后手为赢。最后程序返回一个元祖，第一个表示先手获胜的概率，第二个为后手获胜的概率。\n我们运行一下程序看看结果：\n$ python code.py [0.66436 0.33564]  我们模拟了10万次游戏，程序返回的结果跟数学求解结果十分接近。\n至此文章就完了。在这里我通过一个简单的游戏说明了先手的好处，这告诉我们一个道理，主动出击获得先手优势往往更能够成功。\n","date":1585029650,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1585029650,"objectID":"af1fe0f211643fc79c98f06da6a89d9a","permalink":"https://keris.github.io/zh/post/xianshou-throwing/","publishdate":"2020-03-24T14:00:50+08:00","relpermalink":"/zh/post/xianshou-throwing/","section":"post","summary":"先手意味着主动，从概率上讲获胜的可能性也更大","tags":["先手","概率"],"title":"先手优势","type":"post"},{"authors":["杜利强"],"categories":["C/C++"],"content":"最近参加了一家公司的面试，其中一个问题是：“如何在C语言中生成一个真随机数？” 我当时想到的答案是使用rand()，但rand()返回的其实是一个伪随机数，值的范围是[0, RAND_MAX]。其中，RAND_MAX跟具体的实现有关，也就是说在不同的实现中可能有不同的值。\n以上便是这篇博客的背景，也可以看出自己对随机数生成（RNG）掌握的也不全面，所以试图通过此文弥补一下短板。\n随机数的应用 随机数的应用有很多方面，包括赌博，统计采样，计算机仿真，加密等。\n真随机数与伪随机数 简单来讲，真随机数无法预测，而伪随机数在知道种子值后便能复现。我们先来了解一下生成随机数的两种方法。\n在第一种方法中，我们测量某个被认为是随机的物理现象，然后补偿测量过程中可能的偏差。这种随机源包括空气噪声，热噪声以及其他外部的电磁和量子现象。要产生一个随机数，我们需要获取足够的墒（entropy），但获取速率取决于被测量的物理现象。因此，这种产生随机数的方法是阻塞的（blocking）的，速率受限的。举个例子，在大部分Linux发行版中，伪随机设备文件/dev/random就会阻塞直到从环境获取到足够的墒。由于这种阻塞行为，从/dev/random大批量读以便用随机位填充磁盘经常会很慢，就是因为墒源是阻塞型的。\n在第二种方法中，我们使用计算的方法来生成明显随机值组成的长序列，但实际上结果完全由一个较短的初始值确定，这个初始值也叫种子值。因此，整个看似随机的序列在知道种子值的情况下能够被复现。相比第一种方法，第二种方法是非阻塞（non-blocking）的，所以其生成速率不受外部事件的影响。\n在实际应用中，一些系统采取混合的办法，即在可用的时候采用自然源提供的随机性，否则使用周期性重新设置种子、软件基于的密码安全的伪随机数生成器（CSPRNGs）。\n在这里小结一下：真随机数依赖于收集自然发生的墒，有了足够的墒方可生成随机数，因此是阻塞的，速率有限的；伪随机数使用计算方法产生，在知道种子值后能够复现，非阻塞，速率不受限。\nC语言中的随机数生成 在C语言中，标准库stdlib.h提供了两个函数int rand(void)和void srand(unsigned)来帮助我们生成伪随机数。\nrand()返回一个位于区间[0, RAND_MAX]的整数，RAND_MAX在不同的实现中可能不同，但被保证至少是32767。在调用rand()前，如果我们不设置种子，系统默认我们设置种子为1，即调用了srand(1)，在这种情况下，我们每次运行将会产生同样的值，下面看一个例子：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main() { int num = rand(); printf(\u0026quot;Random number: %d\\n\u0026quot;, num); return 0; }  我们编译以上代码并运行5次：\n$ gcc gen_rand.c \u0026amp;\u0026amp; for i in `seq 1 5`;do ./a.out;done Random number: 16807 Random number: 16807 Random number: 16807 Random number: 16807 Random number: 16807  可以看到，程序在每次运行时均生成了同样的值，这是因为种子值均为1。\n接下来，我们再看看手动设置种子的情况：\n#include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;time.h\u0026gt; int main() { srand((unsigned)time(0)); // use current time as seed int num = rand(); printf(\u0026quot;Random number: %d\\n\u0026quot;, num); return 0; }  这一次，我们将种子设为当前时间。同样地，我们编译并运行5次：\n$ gcc gen_rand1.c \u0026amp;\u0026amp; for i in `seq 1 5`;do ./a.out;done Random number: 7890298 Random number: 7890298 Random number: 7890298 Random number: 7890298 Random number: 7890298  结果看似跟前面没有不同，因为5次调用依然生成了同样的值。其实，这个差别很细微，我们将种子设为当前时间，但只有在重新编译时才能有不同的种子值。因此，两次编译之间生成的值应该不同：\n$ gcc gen_rand1.c \u0026amp;\u0026amp; ./a.out Random number: 12713907 $ gcc gen_rand1.c \u0026amp;\u0026amp; ./a.out Random number: 12747521  C++中的随机数生成 在C++中，我们仍然可以使用rand()和srand()来生成随机数，但这已经不是标准推荐的做法，相反我们应该使用random库提供的工具类来生成随机数。\n下面给出C++版本的代码，运行结果跟C语言别无二致，不再赘述。\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;ctime\u0026gt; using namespace std; int main() { // srand((unsigned)time(nullptr)); // use current time as seed int r = rand(); cout \u0026lt;\u0026lt; \u0026quot;Random number between 0 and \u0026quot; \u0026lt;\u0026lt; RAND_MAX \u0026lt;\u0026lt; \u0026quot; :\u0026quot;; cout \u0026lt;\u0026lt; r \u0026lt;\u0026lt; '\\n'; return 0; }  接下来，我们使用random库提供的工具来生成随机数。random库提供了两种类型的类：\n 均匀分布随机位生成器（URBGs），既包括伪随机数生成和真随机数生成（如果可用） 随机数分布  详细的信息请参见 https://en.cppreference.com/w/cpp/numeric/random。\n下面给出一个使用随机数引擎生成随机数的例子：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;random\u0026gt; using namespace std; int main() { random_device rd; default_random_engine e1(rd()); cout \u0026lt;\u0026lt; e1() \u0026lt;\u0026lt; endl; return 0; }  我们编译以上代码，并运行5次：\n$ g++ main.cc -std=c++17 \u0026amp;\u0026amp; for i in `seq 1 5`;do ./a.out;done 17429349 872858569 1540494345 40333012 95212997  从这里可以看到，我们编译代码一次，多次运行，每次会产生不同的值，序列变得不可预测。\n此外，我们可以将生成的随机数转换到某个闭区间的均匀分布或者正态分布，下面是一个均匀分布的例子：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;iomanip\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;random\u0026gt; #include \u0026lt;map\u0026gt; using namespace std; int main() { random_device rd; default_random_engine e1(rd()); uniform_int_distribution\u0026lt;int\u0026gt; dis(1, 6); cout \u0026lt;\u0026lt; dis.min() \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; dis.max() \u0026lt;\u0026lt; endl; map\u0026lt;int, int\u0026gt; hist; for (int i = 0; i != 10000; i++) { hist[dis(e1)]++; } for (auto[num, count] : hist) { cout \u0026lt;\u0026lt; num \u0026lt;\u0026lt; ' ' \u0026lt;\u0026lt; string(count / 200, '*') \u0026lt;\u0026lt; '\\n'; } return 0; }  编译以上代码并运行，结果如下：\n$ g++ -std=c++17 main.cc \u0026amp;\u0026amp; ./a.out 1 6 1 ******** 2 ******** 3 ******* 4 ******** 5 ******** 6 ********  我们来分析一下。首先，我们输出均匀分布可能生成的最小值和最大值，由于我们考虑的是闭区间[1, 6]上的均匀分布，显然最小值和最大值分别为1和6。然后我们将产生的随机数转换到区间上的某个值，并统计出现次数。最后，我们绘制直方图，从结果来看，每个值出现的次数基本一样多（我们进行了10000次实验！）。\n","date":1584503154,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1584503154,"objectID":"3046ee01433c6af0f1910ac1b8b8c2a1","permalink":"https://keris.github.io/zh/post/random-number-generator/","publishdate":"2020-03-18T11:45:54+08:00","relpermalink":"/zh/post/random-number-generator/","section":"post","summary":"在C/C++中如何生成真随机数","tags":["C/C++","PRNGs","random-number-generator","随机数生成"],"title":"随机数生成","type":"post"},{"authors":["杜利强"],"categories":["C/C++"],"content":"最近看C++编程语言第四版1，阅读到线程这一章，作者在讲解mutex时给出了一个例子，自己照着写了一个发现跑不通，研究后发现对变参模版理解不够，便写了这篇文章。\n首先，我们看看书本上的例子：\nmutex cout_mutex; template\u0026lt;typename Arg, typename... Args\u0026gt; void write(Arg a, Args... tail) { cout_mutex.lock(); cout \u0026lt;\u0026lt; a; write(tail...); cout_mutex.unlock(); }  这个例子旨在说明deadlock，因为我们在write函数中递归调用它，而该函数在进行输出前需要获取互斥变量。为了解除死锁，我们可以使用recursive_mutex，这种类型的mutex允许递归地获取。\n基于以上想法，我写了一个例子来进行测试：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;mutex\u0026gt; using namespace std; recursive_mutex cout_mutex; template\u0026lt;typename Arg, typename... Args\u0026gt; void write(Arg a, Args... tail) { cout_mutex.lock(); cout \u0026lt;\u0026lt; a; write(tail...); cout_mutex.unlock(); } int main(int argc, char* argv[]) { write(\u0026quot;hello,\u0026quot;, \u0026quot;world\u0026quot;); return 0; }  当我尝试编译以上代码却发现报出错误：\n$ g++ test.cc -std=c++17 test.cpp:12:6: error: no matching function for call to 'write' write(tail...); ^~~~~ test.cpp:12:6: note: in instantiation of function template specialization 'write\u0026lt;const char *\u0026gt;' requested here test.cpp:17:6: note: in instantiation of function template specialization 'write\u0026lt;const char *, const char *\u0026gt;' requested here write(\u0026quot;hello,\u0026quot;, \u0026quot;world\u0026quot;); ^ test.cpp:9:7: note: candidate function template not viable: requires at least argument 'a', but no arguments were provided void write(Arg a, Args... tail) { ^ 1 error generated.   在编译时我使用了c++17标准。\n 可以看到，编译器报告了一个错误，对'write'的调用没有匹配的函数，这是为何？\n对于以上的实现，write(\u0026quot;hello,\u0026quot;, \u0026quot;world\u0026quot;)可以拆分成：\n 输出hello,，此时tail包含一个参数即world 输出world，此时tail为空，但write函数至少需要一个参数，所以产生以上错误。  下面我们增加一个接受空参数的write函数来验证：\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;mutex\u0026gt; using namespace std; recursive_mutex cout_mutex; void write() { cout \u0026lt;\u0026lt; \u0026quot;\u0026quot;; // output nothing } template\u0026lt;typename Arg, typename... Args\u0026gt; void write(Arg a, Args... tail) { cout_mutex.lock(); cout \u0026lt;\u0026lt; a; write(tail...); cout_mutex.unlock(); } int main(int argc, char* argv[]) { write(\u0026quot;hello,\u0026quot;, \u0026quot;world\u0026quot;); return 0; }  在进行上述操作后，代码编译通过，运行后输出以下结果：\n$ ./a.out $ hello,world    英文名为：The C++ Programming Language, 4th Edition. 作者为Bjarne Stroustrup. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1584411182,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1584411182,"objectID":"b6852d39bb74bd2ba029595151199bbf","permalink":"https://keris.github.io/zh/post/variadic-template/","publishdate":"2020-03-17T10:13:02+08:00","relpermalink":"/zh/post/variadic-template/","section":"post","summary":"如题","tags":["C++","variadic-template","变参模版"],"title":"一个例子导向的对变参模版的理解","type":"post"},{"authors":["杜利强"],"categories":["机器学习"],"content":"目录    两种类型的错误 模型复杂图 K折交叉验证 学习曲线     两种类型的错误 在选择模型时我们可能犯两种类型的错误，选择过于简单的模型或者过于复杂的模型，前者对应欠拟合，后者对应过拟合。\n欠拟合发生时，模型具有高偏差（high bias），模型不能很好地拟合训练集；过拟合发生时，模型具有高方差（high variance），模型试图对训练数据进行记忆而不是学习其特点，在训练集上表现很好，但在测试集上表现很差。\n下面我们看一个欠拟合的例子：\n   在这个例子中，左边的模型使用一个二次曲线去拟合数据点，而右边则是使用直线去拟合，可以发现直线并不能很好地拟合数据，这就是欠拟合的情形。\n了解了欠拟合，我们来看一个过拟合的例子：\n   Overfitting   在这个例子中，左边的模型使用一个二次曲线去拟合数据点，而右边的模型试图对训练集进行记忆，因而它在训练集上表现很好，但它没有学习到数据的良好属性以很好地泛化到测试集，这就是过拟合的情形。\n前面两个例子属于回归模型，在分类模型中我们也会看到欠拟合和过拟合。\n分类模型中的欠拟合：\n   Underfitting in a Classification Model   分类模型中的过拟合：\n   Overfitting in a Classification Model   最后，我们总结一下：\n   Tradeoff between High Bias and High Variance   模型复杂图 在前面一部分，我们学习了欠拟合和过拟合，在这一部分，我们了解一下模型复杂图，即评估指标随着模型复杂性的提升如何变化。\n   Model Complexity Graph   在上面这个图中，左侧对应欠拟合的情形，此时模型在训练集和验证集上都具有较大的误差；右侧对应过拟合的情形，模型在训练集上具有较小的误差，而在验证集上具有较大的误差；圈起来的那个地方对应刚刚好的情形，此时模型在训练集和验证集上都具有较小的误差，过了这个点，在训练集上的误差持续减小，但在验证集上的误差开始不断变大。\n可能你注意到了一个新概念，验证集，为什么不用测试集？\n在构建机器学习模型时，一个务必要坚持的原则是测试集只能用于最后的模型评估，而不能参与模型训练。那么我们怎么选择一个好的模型，验证集即用于此目的。\n那怎么得到验证集？下图可以给你一个直观的理解：\n   Cross Validation   可以看到，训练集中的一小部分被用来作为验证集，借助验证集我们评估模型是否欠拟合，过拟合或者刚刚好，即验证集用来进行决策，帮助我们选择一个好的模型。最后，测试集用于最终的模型评估。\nK折交叉验证 可能你注意到了，由于从训练集中划出了一小部分作为验证集，这使得训练数据变少了，这会对模型产生不利因素，为此你需要使用K折交叉验证。\n   在上面这个图中，我们有12个样本，其中实心的用于训练，空心的用于测试1。这是一个4折交叉验证，每次其中一折用于测试，剩余三折用于训练。第一次，第一折样本[0,1,2]用于测试；第二次，第二折样本[3,4,5]用于测试；第三次，第三折样本[6,7,8]用于测试；最后一次，最后一折样本[9,10,11]用于测试。 可以看到，经过4折交叉验证，训练集中的所有样本都参与了模型训练。\n接下来，我们看看如何在sklearn中进行K折交叉验证：\nimport numpy as np from sklearn.model_selection import KFold kf = KFold(4) X = np.arange(12) for train_indices, test_indices in kf.split(X): print(train_indices, test_indices)  以上代码产生如下结果：\n[ 3 4 5 6 7 8 9 10 11] [0 1 2] [ 0 1 2 6 7 8 9 10 11] [3 4 5] [ 0 1 2 3 4 5 9 10 11] [6 7 8] [0 1 2 3 4 5 6 7 8] [ 9 10 11]  可以看到，输出结果跟图中的例子完全一致。可能你也注意到了，在这个例子中，每一折都是顺序产生，其实我们可以进行随机选取。\n    同样地，我们看看在sklearn中如何做：\nimport numpy as np from sklearn.model_selection import KFold kf = KFold(4, shuffle=True) X = np.arange(12) for train_indices, test_indices in kf.split(X): print(train_indices, test_indices)  以上代码的结果如下：\n[0 1 2 3 4 6 7 8 9] [ 5 10 11] [ 0 1 2 4 5 6 7 10 11] [3 8 9] [ 0 1 3 4 5 8 9 10 11] [2 6 7] [ 2 3 5 6 7 8 9 10 11] [0 1 4]  可以看到，每一折中的样本不再连续，而是随机抽取的。\n学习曲线 在模型的训练过程中，通过绘制训练误差和验证误差 vs. 参与训练的样本数，我们可以得到两条曲线。\n下面我们来看一个图：\n   Learning Curves   图的左侧是一个高偏差的模型（对应前述的线性模型），即模型欠拟合。在训练样本比较少时，模型能很好地拟合数据，因此训练误差很小，但当评估模型时，由于训练样本较少，模型的表现不会太好，因此验证误差会很大。随着参与训练的样本增加，更多的样本需要拟合，拟合难度增加，训练误差可能会增大一点。此时，由于使用了更多的训练数据，我们会得到一个稍微好点的模型，因此验证误差会减小一点，但不会太多。最后，当更多的数据加入训练，训练误差会增加一点，验证误差会减小一点，它们会越来越靠近。\n图的中间一个刚刚好的模型（对应二次曲线模型）。跟前面描述的情况一样，随着参与训练的样本增加，训练误差会增加，验证误差会减小，最后它们会越来越接近，不同于左侧的情况，验证误差和训练误差会收敛到比较低的位置。\n图的右侧是一个高方差的模型（对应高阶拟合模型），一开始的情况跟前面两个模型的情况一样，但随着参与训练的样本增加，训练误差会增加，但会比较小，因为此时的模型试图对训练集进行记忆，同时验证误差会减小，但会比较大。最后，验证误差和训练误差会收敛，但不会靠近。\n接下来，我们看一个具体的例子：\n样本包含两类数据，如下所示：\n   Scatter Plot of data   针对该数据，我们拟合以下三个模型，看看它们的学习曲线如何：\n 逻辑回归模型 决策树模型 支持向量机模型  import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import learning_curve from sklearn.linear_model import LogisticRegression from sklearn.ensemble import GradientBoostingClassifier from sklearn.svm import SVC # Load data df = pd.read_csv('data.csv') X = np.array(df[['x1', 'x2']]) y = np.array(df['y']) # Fix random seed np.random.seed(55) def randomize(X, Y): permutation = np.random.permutation(Y.shape[0]) X2 = X[permutation,:] Y2 = Y[permutation] return X2, Y2 X2, y2 = randomize(X, y) # Construct three models to draw learning curves estimators = {} # Logistic Regression estimators['LR'] = LogisticRegression() # Decision Tree estimators['DT'] = GradientBoostingClassifier() # Support Vector Machine estimators['SVC'] = SVC(kernel='rbf', gamma=1000) plt.figure(figsize=(12, 6)) for i, m in enumerate(estimators): train_sizes, train_scores, test_scores = learning_curve( estimators[m], X2, y2, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10)) train_scores_mean = np.mean(train_scores, axis=1) # train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) # test_scores_std = np.std(test_scores, axis=1) plt.subplot(1, 3, i + 1) plt.grid() plt.title(\u0026quot;Learning Curves of {}\u0026quot;.format(m)) plt.xlabel(\u0026quot;Training examples\u0026quot;) plt.ylabel(\u0026quot;Score\u0026quot;) plt.plot(train_scores_mean, 'o-', color=\u0026quot;g\u0026quot;, label=\u0026quot;Training score\u0026quot;) plt.plot(test_scores_mean, 'o-', color=\u0026quot;y\u0026quot;, label=\u0026quot;Cross-validation score\u0026quot;) plt.legend(loc=\u0026quot;best\u0026quot;) plt.tight_layout() plt.show()  以上代码将生成三个模型的学习曲线，我们来看一看：\n   根据这些曲线可以得出：\n 逻辑回归模型的训练和测试得分很低。 决策树模型的训练和测试得分很高。 支持向量机模型的训练得分很高，测试得分很低  因此，决策树模型刚刚好，逻辑回归模型欠拟合，而支持向量机模型过拟合。\n同样，我们可以反转这些曲线（因为度量使用的是得分而不是误差）然后与下图进行对比。\n   最后，我们看一下在实际情况中是否也这样，为此我们绘制模型的边界曲线：\n   我们可以看到：\n 逻辑回归模型使用一条直线，这太简单了。在训练集上的效果不太好，因此欠拟合。 决策树模型使用一个方形，拟合的很好，并能够泛化。因此，该模型效果很好。 支持向量机模型实际上在每个点周围都画了一个小圆圈。它实际上是在记住训练集，无法泛化。因此，过拟合。    这里的测试实为验证，不同于最终的测试，后者指模型评估，前者为模型验证。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1584098046,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1584098046,"objectID":"389262a3389827e74e796204507c9f54","permalink":"https://keris.github.io/zh/post/model-selection/","publishdate":"2020-03-13T19:14:06+08:00","relpermalink":"/zh/post/model-selection/","section":"post","summary":"如何选择一个好的机器学习模型，欠拟合和过拟合判断","tags":["欠拟合","过拟合","交叉验证"],"title":"机器学习模型选择","type":"post"},{"authors":[],"categories":["机器学习"],"content":"目录  混淆矩阵 Accuracy Precision Recall F1 score $F_\\beta$ score ROC Curve AUC 回归指标   混淆矩阵     guessed positive guessed negative     positive true positives, #TP false negatives, #FN   negative false positives, #FP true negatives, #TN    假设某个模型在一组样本上的表现如下：\n   其中，蓝色的点为正例（positives)，橙色的点为负例（negatives），则混淆矩阵如下：\n    guessed positive guessed negative     positive 6 1   negative 2 5    Accuracy Accuracy，也就是准确率，表示样本中分类正确所占的比例。\n$$\\text{Accuracy} = \\frac{\\text{#TP} + \\text{#TN}}{\\text{#TP} + \\text{#FP} + \\text{#TN} + \\text{#FN}}$$\n其中:\n #TP表示true positive的数目 #FP表示false positive的数目 #TN表示true negative的数目 #FN表示false negative的数目  对于图1中的模型，准确率计算如下：\n$$\\text{Accuracy} = \\frac{6 + 5}{6 + 1 + 5 + 2} = \\frac{11}{14}= 78.57\\%$$\n准确率在数据偏斜的情况下将不再适用。比如在下图所示的例子中：\n   模型需要找出良好的交易，但我们的样本中绝大部分都是良好的交易，这导致无论我们的模型如何，它都具有很高的准确率，如图中所示为99.83%！，此时准确率将不能够评估我们的模型。\nPrecision Precision即精度，表示在所有预测为正例的样本中有多少是真正例。\n$$\\text{Precision} = \\frac{\\text{#TP}}{\\text{#TP} + \\text{#FP}}$$\n对于图1中的模型，精度计算如下：\n$$\\text{Precision} = \\frac{6}{6 + 2} = \\frac{6}{8} = 75\\%$$\nRecall Recall即为召回率，所有的正例样本中真正例所占的比例：\n$$\\text{Recall} = \\frac{\\text{#TP}}{\\text{#TP} + \\text{#FN}}$$\n对于图1中的模型，召回率计算如下：\n$$\\text{Recall} = \\frac{6}{6 + 1} = \\frac{6}{7} = 85.71\\%$$\nF1 score F1-score定义如下：\n$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n可以看到F1-score综合了精度和召回率，为精度和召回率的调和平均：在召回率不变的条件下，精度越高，F1-score越大；在精度不变的条件下，召回率越高，F1-score越大。\n$F_\\beta$ score $F_\\beta-\\text{score}$定义为：\n$$F_\\beta = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}$$\n可以看出来，当$\\beta = 1$时即为F1 score。\n下面我们看下$F_\\beta$随$\\beta$变化的情况：\n 当$\\beta = 0$时，通过计算不难得出$F_0 = \\text{Precision}$; 相反，当$\\beta$取很大的值时，$F_\\beta$将趋近召回率。  小结：当$\\beta$取较小的值时，精度比召回率重要，如$F_{0.5}$ score；当$\\beta$取较大值时召回率比精度重要，如$F_2$ score。\nROC Curve ROC Curve的全称为Receiver Operating Characteristic Curve，即受试者工作特性缺陷，它刻画了模型在所有分类阈值下的表现。\n要绘制该曲线，我们需要计算以下两个参数：\n True Positive Rate, TPR False Positive Rate, FPR  TPR，表示所有的正例样本中真正例的占比，不难发现，这其实跟召回率一个概念：\n$$\\text{TPR} = \\frac{\\text{#TP}}{\\text{#TP} + \\text{#FN}}$$\nFPR，表示所有的负例样本中假正例的占比：\n$$\\text{FPR} = \\frac{\\text{#FP}}{\\text{#FP} + \\text{#TN}}$$\n在不同的分类阈值下绘制TPR vs. FPR便得到ROC曲线：\n   AUC AUC的全称为Area Under a ROC Curve，即ROC曲线下的面积。\n   回归指标  Mean Absolute Error (MAE)  MAE即平均绝对误差，假设我们有一组样本${x_i, y_i}, i = 1, 2, \\cdots, n$，模型的输出为$\\text{preds} = [\\hat{y}_1, \\hat{y}_2, \\cdots, \\hat{y}_n]$，则MAE计算如下：\n$$\\text{MAE} = \\frac{1}{n}\\sum_i^n |y_i - \\hat{y}_i|$$\n借助sklearn，我们可以很方便地计算MAE：\nfrom sklearn.metrics import mean_absolute_error from sklearn.linear_model import LinearRegression clf = LinearRegression() preds = clf.fit(X, y) error = mean_absolute_error(y, preds)  从公式来看，MAE不可导，使得不能采用梯度下降方法进行优化，此时我们可以借助均方误差。\nMean Squared Error (MSE)  MSE即均方误差，定义如下：\n$$\\text{MSE} = \\frac{1}{n}\\sum_i^n (y_i - \\hat{y}_i)^2$$\n同样地，MSE通过sklearn可以很方便地进行计算：\nfrom sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression clf = LinearRegression() preds = clf.fit(X, y) error = mean_squared_error(y, preds)  R2 Score  R2分数通过将现有的模型与最简单的可能模型相比得出。\n比如，我们要拟合一堆数据点，那最简单的可能模型是什么？那就是我们取所有值的平均值，然后画一条水平线。此时我们可以计算出这个模型的均方误差大于线性回归模型的均方误差，如下图所示：\n   R2分数定义为：\n$$\\text{R2} = 1 - \\frac{\\text{MSE of 当前模型}}{\\text{MSE of 最简单的可能模型}}$$\n以上图为例，分子为线性回归模型（右侧）的均方误差，分母为简单模型（左侧）的均方误差。\n由于最简单的可能模型的均方误差大于线性回归模型的均方误差，我们在相比之后得到一个0到1之间的数字，从而R2得分也是一个0到1之间的数，并且有以下结论：\n R2越接近0，我们的模型越接近最简单的可能模型，此时模型是一个bad model 相反，R2越接近1，说明我们模型相比最简单的可能模型具有更小的均方误差，此时模型是一个good model  在sklearn中，我们如下计算R2得分：\n\u0026gt;\u0026gt;\u0026gt; from sklearn.metrics import r2_score \u0026gt;\u0026gt;\u0026gt; y_true = [1, 2, 4] \u0026gt;\u0026gt;\u0026gt; y_pred = [1.3, 2.5, 3.7] \u0026gt;\u0026gt;\u0026gt; r2_score(y_true, y_pred) \u0026gt;\u0026gt;\u0026gt; 0.9078571428571429  ","date":1583925611,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1583925611,"objectID":"9b5a8c97e2b1a3c38fd669a38e165a5e","permalink":"https://keris.github.io/zh/post/metrics/","publishdate":"2020-03-11T19:20:11+08:00","relpermalink":"/zh/post/metrics/","section":"post","summary":"了解机器学习模型的各种评估指标","tags":["评估指标"],"title":"评估指标","type":"post"},{"authors":[],"categories":["金融风控"],"content":"目录  信用评分卡模型  评分卡模型划分 如何利用评分卡对用户进行评分？ 小结   评分卡模型的开发  评分卡模型开发流程  数据获取 EDA 数据预处理 数据清洗 变量分箱  无监督分箱 有监督分箱 小结         信用评分卡模型 信用评分卡模型是一种最常见的金融风控手段。它是指根据客户的各种属性和行为数据构建一个信用评分模型，对客户进行信用评分，并据此决定是否给予授信以及授信的额度和利率，从而识别和减少在金融交易中存在的交易风险。\n以下为评分卡模型的示意图：\n  Credit Score Card Demo   评分卡模型划分 在不同的业务阶段我们可以构建不同的评分卡模型。根据借贷时间，评分卡模型可以划分为以下三种：\n 贷前：申请评分卡（Application score card），又称为A卡 贷中：行为评分卡（Behavior score card），又称为B卡 贷后：催收评分卡（Collection score card），又称为C卡  如何利用评分卡对用户进行评分？ 一个用户的评分等于基准分加上对客户各个属性的评分。对前面给出的示意图中的例子而言：\n客户评分 = 基准分 + 年龄评分 + 性别评分 + 婚姻状况评分 + 学历评分 + 月收入评分  现有一客户，其年龄为27岁，性别男，已婚，本科学历，月收入为10k，那么他的评分为：\n223(基准分) + 8(年龄评分) + 4(性别评分) + 8(婚姻状况评分) + 8(学历评分) + 13(月收入评分) = 264  小结 以上就是评分卡模型的具体用法。在前面的例子中，不难发现以下三个问题：\n 如何选择用户的属性？ 评分卡模型采用的是对属性的分段进行评分，那么如何进行有效的分段？ 如何对每个分段进行评分？  评分卡模型的开发 评分卡模型开发流程 信用评分卡模型的开发一般包括数据获取，EDA，数据预处理，变量筛选，模型开发及评估，生成评分卡模型，模型上线及监测。典型的开发流程图如下：\n  Credit Score Card Flowchart   数据获取 数据主要有两个来源：\n 金融机构自有数据：如用户的年龄，户籍，性别，收入，负债比，在本机构的借款和还款行为等 第三方数据：如用户在其他机构的借贷行为，用户的消费行为数据等  EDA EDA为Exploratory Data Analysis的简写，即探索性数据分析。这个阶段旨在了解数据的主要特性，如字段的缺失情况，异常值情况，中位数，分布等。最后制定一个合理的数据预处理方案。\n数据预处理 数据预处理主要包括数据清洗，变量分箱和WOE编码。\n数据清洗 数据清洗主要是对数据中的缺失值和异常值进行处理。一般我们选择删除缺失率超过某个阈值1（如30%，50%等）的变量。\n变量分箱 所谓的分箱定义如下：\n 对连续变量进行分段以离散化 将离散变量的多个状态进行合并，减少离散变量的状态数  通过分箱操作，我们达到了对变量分段的目的。\n常见的分箱类型如下：\n  常见的变量分箱类型   无监督分箱 无监督分箱主要包括三类：\n 等频分箱：把自变量的值按从小到大排序，将自变量的取值个数等分为k部分，每部分作为一个分箱 等距分箱：把自变量的值按从小到大排序，将自变量的取值范围分为k个等距的区间，每个区间作为一个分箱 聚类分箱：用k-means等聚类法将自变量分为k类，但需要在聚类过程中保证分箱的有序性  小结： 无监督分箱仅考虑了各个变量自身的结构，并没有考虑自变量与目标变量之间的关系，因此这种分箱方法不一定会带来模型性能的提升。\n有监督分箱 有监督分箱主要包括Split分箱和Merge分箱。\n Split分箱是一种自上而下的分箱方法，其和决策树比较相似，切分点的选择指标主要有entropy， gini指数和IV值等。 Merge分箱是一种自下而上的分箱方法，常见的merge分箱为ChiMerge分箱。  ChiMerge分箱的基本思想是：如果两个相邻区间具有相似的类分布，则合并它们，否则保持不变。ChiMerge通常采用卡方值来度量两相邻区间的类分布的相似性。\n计算卡方值的公式如下：\n$$ \\chi^2 = \\sum_{i=1}^2 \\sum_{j=1}^k \\frac{(A_{ij} - E_{ij})^2}{E_{ij}} $$\n其中：\n$k =$ 类别数目\n$A_{ij} =$ 第 $i$ 个区间中第$j$个类别的样本数目\n$E_{ij} = A_{ij}$的期望频率$=\\frac{R_i \\times C_j}{N}$，$R_i =$第$i$区间的样本数目$= \\sum_{j=1}^k A_{ij}$，$C_j = $第$j$个类别的样本数目$= \\sum_{i=1}^2 A_{ij}$\nChiMerge算法包含一个初始化步和一个自底向上的合并过程。\n 初始化：将需要离散化的属性的值按从小到大排序，将每个样本作为一个区间 合并区间  计算每对相邻区间的$\\chi^2$ 合并具有最小$\\chi^2$的相邻区间   如果所有相邻区间对的$\\chi^2$超过了$\\chi^2-threshold$，则停止，否则回到第2步  如何确定$\\chi^2-threshold$？为此我们需要选择一个想要的显著性水平和自有度，然后通过查表或者公式获得对应的$\\chi^2$。其中自由度为类别数目 - 1，例如，当有3个类别时，自由度为2。\n除了使用$\\chi^2-threshold$，还可以指定最小区间数和最大区间数来确保不会产生太少或者太多的区间。\n在金融风控中，当我们应用ChiMerge时，在初始化时往往有以下操作：\n 连续值按升序排列，离散值首先转化为坏客户的占比，然后再按升序排列 为了减少计算量，对于状态数大于某一阈值（建议为100）的变量，利用等频分箱进行粗分箱 若有缺失值，则缺失值单独作为一个分箱  在分箱完后还会做进一步的处理：\n 对于坏客户占比为0或者1的分箱进行合并（一个分箱内不能全为好客户或者坏客户） 对于样本占比超过95%的箱子进行删除 检查缺失分箱的坏客户比例是否和非缺失分箱相等，如果相等，进行合并  小结  分箱可以有效处理缺失值和异常值 分箱后数据和模型会更稳定 分箱可以简化逻辑回归模型，降低过拟合风险，提高泛化能力 分箱将特征统一变换为类别型变量 分箱后变量才可以使用标准的评分卡格式    阈值的选择需要根据实际情况确定。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1583227688,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1583227688,"objectID":"1cee129b977dc9746afbf84680375dc3","permalink":"https://keris.github.io/zh/post/credit-score-card/","publishdate":"2020-03-03T17:28:08+08:00","relpermalink":"/zh/post/credit-score-card/","section":"post","summary":"学习信用评分卡模型的全流程","tags":["评分卡"],"title":"信用评分卡模型","type":"post"},{"authors":[],"categories":[],"content":"RFM是一种用于分析客户价值的方法，常用于营销。其中RFM代表三个维度：\n Recency 表示最近一次客户购买的时间 Frequency 表示在统计周期内客户购买的次数 Monetary Value 表示统计周期内客户消费的总金额  接下来，我们使用RFM模型分析一个真实的在线购物数据。\n# Import libraries import pandas as pd import matplotlib.pyplot as plt import squarify from datetime import timedelta import seaborn as sns  # Read dataset online = pd.read_csv('data.csv', encoding='ISO-8859-1') online.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country     0 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 12/1/2010 8:26 2.55 17850.0 United Kingdom   1 536365 71053 WHITE METAL LANTERN 6 12/1/2010 8:26 3.39 17850.0 United Kingdom   2 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 12/1/2010 8:26 2.75 17850.0 United Kingdom   3 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 12/1/2010 8:26 3.39 17850.0 United Kingdom   4 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 12/1/2010 8:26 3.39 17850.0 United Kingdom     online.dtypes  InvoiceNo object StockCode object Description object Quantity int64 InvoiceDate object UnitPrice float64 CustomerID float64 Country object dtype: object  # Convert InvoiceDate from object to datetime format online['InvoiceDate'] = pd.to_datetime(online['InvoiceDate'])  online.dtypes  InvoiceNo object StockCode object Description object Quantity int64 InvoiceDate datetime64[ns] UnitPrice float64 CustomerID float64 Country object dtype: object  # Check how many rows and columns online.shape  (541909, 8)  # Count transactions that don't have a customer id print('{:,} transactions don\\'t have a customer id' .format(online[online.CustomerID.isnull()].shape[0]))  135,080 transactions don't have a customer id  # Check invoice date range print('Transactions timeframe from {} to {}' .format(online.InvoiceDate.min(), online.InvoiceDate.max()))  Transactions timeframe from 2010-12-01 08:26:00 to 2011-12-09 12:50:00  # Drop NA values from online online.dropna(inplace=True)  # Group data by CustomerID # Create TotalSum column for online dataset online['TotalSum'] = online['Quantity'] * online['UnitPrice'] # Create snapshot date snapshot_date = online.InvoiceDate.max() + timedelta(days=1) print(snapshot_date) # Group by CustomerID data = online.groupby(['CustomerID']).agg({ 'InvoiceDate': lambda x: (snapshot_date - x.max()).days, 'InvoiceNo': 'count', 'TotalSum': 'sum' }) data.rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'TotalSum': 'MonetaryValue'}, inplace=True)  2011-12-10 12:50:00  # Peek first 5 rows data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue   CustomerID        12346.0 326 2 0.00   12347.0 2 182 4310.00   12348.0 75 31 1797.24   12349.0 19 73 1757.55   12350.0 310 17 334.40     # Check how many rows and columns data.shape  (4372, 3)  在这里我们可以看到，根据CutomerID进行分组后，我们聚合生成了每个客户最近一次购买的时间，购买频率和消费额度，共4372条记录。接下来，我们需要对这三个维度进行打分，这可以通过.qcut()来进行。但在此之前，我们先看一看特征的分布情况。\n# Plot RFM distributions plt.figure(figsize=(12,10)) # Plot distribution of R plt.subplot(3, 1, 1) sns.distplot(data['Recency']) # Plot distribution of F plt.subplot(3, 1, 2) sns.distplot(data['Frequency']) # Plot distribution of M plt.subplot(3, 1, 3) sns.distplot(data['MonetaryValue'])  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1199779b0\u0026gt;  # Calculate R and F groups # Create labels for Recency and Frequency r_labels = range(4, 0, -1) f_labels = range(1, 5) # Assign these labels to 4 equal percentil groups r_groups = pd.qcut(data['Recency'], q=4, labels=r_labels) f_groups = pd.qcut(data['Frequency'], q=4, labels=f_labels) # Create new columns R and F data = data.assign(R=r_groups.values, F=f_groups.values) data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F   CustomerID          12346.0 326 2 0.00 1 1   12347.0 2 182 4310.00 4 4   12348.0 75 31 1797.24 2 2   12349.0 19 73 1757.55 3 3   12350.0 310 17 334.40 1 1     # Create labels form MonetaryValue m_labels = range(1, 5) m_groups = pd.qcut(data['MonetaryValue'], q=4, labels=m_labels) data = data.assign(M=m_groups.values)  data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F M   CustomerID           12346.0 326 2 0.00 1 1 1   12347.0 2 182 4310.00 4 4 4   12348.0 75 31 1797.24 2 2 4   12349.0 19 73 1757.55 3 3 4   12350.0 310 17 334.40 1 1 2     # Concat RFM quartile values to create RFM segments def join_rfm(x): return str(x['R']) + str(x['F']) + str(x['M']) data['RFM_segment_concat'] = data.apply(join_rfm, axis=1) rfm = data rfm.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F M RFM_segment_concat   CustomerID            12346.0 326 2 0.00 1 1 1 111   12347.0 2 182 4310.00 4 4 4 444   12348.0 75 31 1797.24 2 2 4 224   12349.0 19 73 1757.55 3 3 4 334   12350.0 310 17 334.40 1 1 2 112     # Count num of unique segments rfm_count_unique = rfm.groupby('RFM_segment_concat')['RFM_segment_concat'].nunique() print(rfm_count_unique.sum())  62  以上结果显示，将RFM连接在一起这种方法共形成62个划分，但划分太多了不能用于实际应用。下面我们尝试将这三个值相加。\n# Calculate RFM_Score rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1) print(rfm['RFM_Score'].head())  CustomerID 12346.0 3.0 12347.0 12.0 12348.0 8.0 12349.0 10.0 12350.0 4.0 Name: RFM_Score, dtype: float64  # Define rfm_level function def rfm_level(df): if df['RFM_Score'] \u0026gt;= 9: return 'Can\\'t Loose Them' elif ((df['RFM_Score'] \u0026gt;= 8) and (df['RFM_Score'] \u0026lt; 9)): return 'Champions' elif ((df['RFM_Score'] \u0026gt;= 7) and (df['RFM_Score'] \u0026lt; 8)): return 'Loyal' elif ((df['RFM_Score'] \u0026gt;= 6) and (df['RFM_Score'] \u0026lt; 7)): return 'Potential' elif ((df['RFM_Score'] \u0026gt;= 5) and (df['RFM_Score'] \u0026lt; 6)): return 'Promising' elif ((df['RFM_Score'] \u0026gt;= 4) and (df['RFM_Score'] \u0026lt; 5)): return 'Needs Attention' else: return 'Require Activation' # Create a new variable RFM_Level rfm['RFM_Level'] = rfm.apply(rfm_level, axis=1) rfm.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F M RFM_segment_concat RFM_Score RFM_Level   CustomerID              12346.0 326 2 0.00 1 1 1 111 3.0 Require Activation   12347.0 2 182 4310.00 4 4 4 444 12.0 Can't Loose Them   12348.0 75 31 1797.24 2 2 4 224 8.0 Champions   12349.0 19 73 1757.55 3 3 4 334 10.0 Can't Loose Them   12350.0 310 17 334.40 1 1 2 112 4.0 Needs Attention     # Calculate average values for each RFM_Level, and return a size of each segment rfm_level_agg = rfm.groupby('RFM_Level').agg({ 'Recency': 'mean', 'Frequency': 'mean', 'MonetaryValue': ['mean', 'count'] }).round(1) # Print the aggregate dataset print(rfm_level_agg)   Recency Frequency MonetaryValue mean mean mean count RFM_Level Can't Loose Them 25.2 195.1 4130.3 1690 Champions 62.7 57.0 974.7 467 Loyal 78.8 39.7 724.2 447 Needs Attention 174.5 13.9 227.1 391 Potential 94.3 28.5 491.8 468 Promising 153.0 21.2 346.8 517 Require Activation 264.8 7.8 109.1 392  从以上结果我们可以看到，约60%的客户属于优质客户（前三类用户）。\n# Visualize segments rfm_level_agg.columns = rfm_level_agg.columns.droplevel() rfm_level_agg.columns = ['RecencyMean', 'FrequencyMean', 'MonetaryMean', 'Count']  # Create our plot and resize it fig = plt.gcf() ax = fig.add_subplot() fig.set_size_inches(16, 9) squarify.plot(sizes=rfm_level_agg['Count'], label=list(rfm_level_agg.index), alpha=.6) plt.title('RFM Segments', fontsize=18, fontweight='bold') plt.axis('off') plt.show()  ","date":1582702890,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1582702890,"objectID":"ef833db58e604d147197982b1096f887","permalink":"https://keris.github.io/zh/post/what-is-rfm-model/","publishdate":"2020-02-26T15:41:30+08:00","relpermalink":"/zh/post/what-is-rfm-model/","section":"post","summary":"简要说明RFM模型并给出一个实际的使用例子","tags":["RFM"],"title":"RFM模型及实践","type":"post"},{"authors":["杜利强"],"categories":["机器学习"],"content":"目录  术语  Logistic function Odds Logit function   逻辑回归  代价函数求导     术语 逻辑回归涉及到以下术语：\n Logistic function Odds Logit  Logistic function 逻辑回归中的 Logistic 正是出于 Logistic function ，它是一种 sigmoid function ，其接受任意实值，输出一个0到1之间的值。 标准的 logistic funtion 定义如下：\n$$\\sigma(z) = \\frac{e^z}{e^z + 1} = \\frac{1}{1 + e^{-z}}$$\n如下是它在区间$[-6, 6]$之间的图像：\n\n在逻辑回归中，我们使用对数几率（log odds），并假定它是输入特征的线性组合：\n$$z = \\ln \\frac{p(x)}{1 - p(x)}= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = \\theta^T \\cdot x$$\n由上式我们可以得到，\n$$p(x) = \\sigma(z) = \\frac{1}{1 + e^{-\\theta^T \\cdot x}}$$\n在逻辑回归模型中，这里的 $p(x)$ 为因变量在成功情形下的概率，即 $p(y=1 \\mid x)$。\nOdds 如果 $p$ 表示一个事件发生的概率，那么odds定义为\n$$\\text{odds} = \\frac{p}{1 - p}$$ 也就是说，odds为发生的概率除以不发生的概率，亦可以说为成功的概率除以失败的概率。\nLogit function Logit为Log odds, logit function 定义为 logistic function的逆，即 $g = \\sigma^{-1}$。显而易见，我们有\n$$g(p(x)) = \\sigma_{-1}(p(x)) = \\text{logit}\\,p(x) = \\ln(\\frac{p(x)}{1 - p(x)}) = \\theta^T \\cdot x$$\n逻辑回归 逻辑回归是一个重要的机器学习算法，其目标是基于给定的数据$x$输出随机变量$y$为0或1的概率。\n考虑由$\\theta$参数化的线性模型，\n$$h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T \\cdot x}} = \\text{Pr}(y = 1 \\mid x;\\theta)$$\n从而，$\\text{Pr}(y=0 \\mid x;\\theta) = 1 - h_\\theta(x)$。\n因为$y \\in \\{0, 1 \\}$，我们有\n$$\\text{Pr}(y \\mid x;\\theta) = h_\\theta(x)^y (1 - h_\\theta(x))^{1 - y} $$\n似然函数为\n$$\\begin{aligned} L(\\theta \\mid x) \u0026amp;= \\Pr(Y\\mid X;\\theta) \\\\\n\u0026amp;= \\prod_i \\Pr(y^{(i)} \\mid x^{(i)};\\theta) \\\\\n\u0026amp;= \\prod_i h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{1-y^{(i)}} \\end{aligned}$$\n一般地，我们最大化对数似然函数，\n$$\\log L(\\theta \\mid x) = \\sum_{i=1}^{m}\\log \\Pr(y^{(i)} \\mid x^{(i)};\\theta)$$\n定义代价函数如下：\n$$J(\\theta) = -\\frac{1}{m} \\log L(\\theta \\mid x) = -\\frac{1}{m} \\sum_i^m (y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)})(1 - h_\\theta(x^{(i)})))$$\n容易看到，我们最大化对数似然函数就是最小化代价函数 $J(\\theta)$。在机器学习中，我们使用梯度下降最小化代价函数。\n代价函数求导 下面我们对代价函数$J(\\theta)$对$\\theta$进行求导，我们先考虑在一个样本上的代价函数\n$$J_1(\\theta) = -y \\log h_\\theta(x) - (1 - y)(1 - h_\\theta(x))$$\n现在对$J_1(\\theta)$对$\\theta$进行求导：\n注意到$\\frac{d\\sigma(z)}{dz} = \\sigma(z) (1 - \\sigma(z))$, 我们有\n$$\\begin{aligned}\\frac{\\partial}{\\partial \\theta_j} J_1(\\theta) \u0026amp;= -y \\frac{1}{h_\\theta(x)} h_\\theta(x) (1 - h_\\theta(x)) x_j - (1 - y) \\frac{1}{1 - h_\\theta(x)} (-1) h_\\theta(x) (1 - h_\\theta(x)) x_j \\\\ \u0026amp;= (h_\\theta(x) - y) x_j\\end{aligned} $$\n有了以上结果，我们有\n$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m}\\sum_i^m [h_\\theta(x^{(i)}) - y^{(i)}]x_j^{(i)}$$\n","date":1578991642,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1579496437,"objectID":"aebccbb495f02050f938f27e4bd41ef9","permalink":"https://keris.github.io/zh/post/logistic-regression/","publishdate":"2020-01-14T16:47:22+08:00","relpermalink":"/zh/post/logistic-regression/","section":"post","summary":"理解逻辑回归中涉及的术语以及从最大似然到最小化代价函数","tags":["logistic-regression","逻辑回归"],"title":"逻辑回归（Logistic Regression）","type":"post"},{"authors":[],"categories":[],"content":"在git的日常使用中更改提交的作者名和邮箱是一个十分常见的操作。例如，当你克隆了一个项目，如果你没有进行任何设置，此时提交的作者和邮件将使用全局选项，这可能不是你所要的。\n通过本文你将了解：\n 如何配置git使用的提交者用户名和邮箱 如何更改最近一次提交的作者名和邮件 批量更改提交的作者名和邮件  配置提交者用户名和邮箱 你有两种方式进行设置，一是全局配置，二是为每个repo单独配置。\n全局地更改提交者用户名和邮箱 使用git config并--global选项进行全局设置，如下所示：\n$ git config --global user.name \u0026quot;Du Liqiang\u0026quot; $ git config --global user.email \u0026quot;dlq137@gmail.com\u0026quot;  设置完毕后，后续的提交将使用以上提供的信息。\n为某个repo单独配置 全局选项可能并不适用于某个repo，此时就需要单独设置，使用git config但省略--global选项，如下：\n$ git config user.name \u0026quot;Du Liqiang\u0026quot; $ git config user.email \u0026quot;dlq137@gmail.com\u0026quot;  这里的设置将覆盖全局选项，并且只应用于当前的repo。\n更改最近一次提交的作者用户名和邮箱 如果你刚做了一次提交，发现用户名和邮箱并不是所要的，你可以使用--amend选项重新提交：\n$ git commit --amend --author=\u0026quot;Du Liqiang \u0026lt;dlq137@gmail.com\u0026gt;\u0026quot;  更改多次提交的作者用户名和邮箱 这个时候我们需要借助强大的rebase命令，首先我们找到上一次“好”的提交1，并假设其提交hash为0ad14fa5，执行：\n$ git rebase -i -p 0ad14fa5  此时我们会进入一个编辑器，将那些需要编辑的提交全标记为edit，接下来git会指导你完成每次提交的编辑，你需要做的就是执行：\n$ git commit --amend --author=\u0026quot;Du Liqiang \u0026lt;dlq137@gmail.com\u0026gt; --no-edit $ git rebase --continue  使用git filter-branch批量更改 除了以上交互式的更改方法，另一种方法是借助git的filter-branch命令，其允许你使用一个script批量处理大量的提交。\n如下命令筛选提交邮箱为WRONG_EMAIL的提交，并将其用户名和邮箱分别设置为NEW_NAME和NEW_EMAIL对应的值。\n$ git filter-branch --env-filter ' WRONG_EMAIL=\u0026quot;wrong@example.com\u0026quot; NEW_NAME=\u0026quot;New Name Value\u0026quot; NEW_EMAIL=\u0026quot;correct@example.com\u0026quot; if [ \u0026quot;$GIT_COMMITTER_EMAIL\u0026quot; = \u0026quot;$WRONG_EMAIL\u0026quot; ] then export GIT_COMMITTER_NAME=\u0026quot;$NEW_NAME\u0026quot; export GIT_COMMITTER_EMAIL=\u0026quot;$NEW_EMAIL\u0026quot; fi if [ \u0026quot;$GIT_AUTHOR_EMAIL\u0026quot; = \u0026quot;$WRONG_EMAIL\u0026quot; ] then export GIT_AUTHOR_NAME=\u0026quot;$NEW_NAME\u0026quot; export GIT_AUTHOR_EMAIL=\u0026quot;$NEW_EMAIL\u0026quot; fi ' --tag-name-filter cat -- --branches --tags  参考  https://www.git-tower.com/learn/git/faq/change-author-name-email    该次提交之前的提交具有正确的用户名和邮箱，其后的需要进行更改。 \u0026#x21a9;\u0026#xfe0e;\n  ","date":1573120043,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1573120043,"objectID":"9f6fd127ef1ad95f23bfd5dc1c97aaa2","permalink":"https://keris.github.io/zh/post/change-author-name-email/","publishdate":"2019-11-07T17:47:23+08:00","relpermalink":"/zh/post/change-author-name-email/","section":"post","summary":"在git的日常使用中更改提交的作者名和邮箱是一个十分常见的操作。例如，当你克隆了一个项目，如果你没有进行任何设置，此时提交的作者和邮件将使用全局选项，这可能不是你所要的。\n","tags":["git","change-author"],"title":"更改git提交中的作者和邮件信息","type":"post"}]
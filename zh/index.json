[{"authors":["admin"],"categories":null,"content":"我是杜利强，现在是友信金服科技有限公司的一名软件工程师，曾在新浪微博担任搜索算法工程师，在诺禾致源担任生物信息软件开发工程师。我的兴趣包括互联网金融风控、信息检索排序、推荐系统。\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"zh","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://keris.github.io/zh/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/authors/admin/","section":"authors","summary":"我是杜利强，现在是友信金服科技有限公司的一名软件工程师，曾在新浪微博担任搜索算法工程师，在诺禾致源担任生物信息软件开发工程师。我的兴趣包括互联网金融风控、信息检索排序、推荐系统。","tags":null,"title":"杜利强","type":"authors"},{"authors":[],"categories":[],"content":"目录  模型选择  两种类型的错误 模型复杂图 K折交叉验证 学习曲线     模型选择 两种类型的错误 在选择模型时我们可能犯两种类型的错误，选择过于简单的模型或者过于复杂的模型，前者对应欠拟合，后者对应过拟合。\n欠拟合发生时，模型具有高偏差（high bias），模型不能很好地拟合训练集；过拟合发生时，模型具有高方差（high variance），模型试图对训练数据进行记忆而不是学习其特点，在训练集上表现很好，但在测试集上表现很差。\n下面我们看一个欠拟合的例子：\n   在这个例子中，左边的模型使用一个二次曲线去拟合数据点，而右边则是使用直线去拟合，可以发现直线并不能很好地拟合数据，这就是欠拟合的情形。\n了解了欠拟合，我们来看一个过拟合的例子：\n   Overfitting   在这个例子中，左边的模型使用一个二次曲线去拟合数据点，而右边的模型试图对训练集进行记忆，因而它在训练集上表现很好，但它没有学习到数据的良好属性以很好地泛化到测试集，这就是过拟合的情形。\n前面两个例子属于回归模型，在分类模型中我们也会看到欠拟合和过拟合。\n分类模型中的欠拟合：\n   Underfitting in a Classification Model   分类模型中的过拟合：\n   Overfitting in a Classification Model   最后，我们总结一下：\n   Tradeoff between High Bias and High Variance   模型复杂图 在前面一部分，我们学习了欠拟合和过拟合，在这一部分，我们了解一下模型复杂图，即评估指标随着模型复杂性的提升如何变化。\n   Model Complexity Graph   在上面这个图中，左侧对应欠拟合的情形，此时模型在训练集和验证集上都具有较大的误差；右侧对应过拟合的情形，模型在训练集上具有较小的误差，而在验证集上具有较大的误差；圈起来的那个地方对应刚刚好的情形，此时模型在训练集和验证集上都具有较小的误差，过了这个点，在训练集上的误差持续减小，但在验证集上的误差开始不断变大。\n可能你注意到了一个新概念，验证集，为什么不用测试集？\n在构建机器学习模型时，一个务必要坚持的原则是测试集只能用于最后的模型评估，而不能参与模型训练。那么我们怎么选择一个好的模型，验证集即用于此目的。\n那怎么得到验证集？下图可以给你一个直观的理解：\n   Cross Validation   可以看到，训练集中的一小部分被用来作为验证集，借助验证集我们评估模型是否欠拟合，过拟合或者刚刚好，即验证集用来进行决策，帮助我们选择一个好的模型。最后，测试集用于最终的模型评估。\nK折交叉验证 可能你注意到了，由于从训练集中划出了一小部分作为验证集，这使得训练数据变少了，这会对模型产生不利因素，为此你需要使用K折交叉验证。\n   在上面这个图中，我们有12个样本，其中实心的用于训练，空心的用于测试1。这是一个4折交叉验证，每次其中一折用于测试，剩余三折用于训练。第一次，第一折样本[0,1,2]用于测试；第二次，第二折样本[3,4,5]用于测试；第三次，第三折样本[6,7,8]用于测试；最后一次，最后一折样本[9,10,11]用于测试。 可以看到，经过4折交叉验证，训练集中的所有样本都参与了模型训练。\n接下来，我们看看如何在sklearn中进行K折交叉验证：\nimport numpy as np from sklearn.model_selection import KFold kf = KFold(4) X = np.arange(12) for train_indices, test_indices in kf.split(X): print(train_indices, test_indices)  以上代码产生如下结果：\n[ 3 4 5 6 7 8 9 10 11] [0 1 2] [ 0 1 2 6 7 8 9 10 11] [3 4 5] [ 0 1 2 3 4 5 9 10 11] [6 7 8] [0 1 2 3 4 5 6 7 8] [ 9 10 11]  可以看到，输出结果跟图中的例子完全一致。可能你也注意到了，在这个例子中，每一折都是顺序产生，其实我们可以进行随机选取。\n    同样地，我们看看在sklearn中如何做：\nimport numpy as np from sklearn.model_selection import KFold kf = KFold(4, shuffle=True) X = np.arange(12) for train_indices, test_indices in kf.split(X): print(train_indices, test_indices)  以上代码的结果如下：\n[0 1 2 3 4 6 7 8 9] [ 5 10 11] [ 0 1 2 4 5 6 7 10 11] [3 8 9] [ 0 1 3 4 5 8 9 10 11] [2 6 7] [ 2 3 5 6 7 8 9 10 11] [0 1 4]  可以看到，每一折中的样本不再连续，而是随机抽取的。\n学习曲线 在模型的训练过程中，通过绘制训练误差和验证误差 vs. 参与训练的样本数，我们可以得到两条曲线。\n下面我们来看一个图：\n   Learning Curves   图的左侧是一个高偏差的模型（对应前述的线性模型），即模型欠拟合。在训练样本比较少时，模型能很好地拟合数据，因此训练误差很小，但当评估模型时，由于训练样本较少，模型的表现不会太好，因此验证误差会很大。随着参与训练的样本增加，更多的样本需要拟合，拟合难度增加，训练误差可能会增大一点。此时，由于使用了更多的训练数据，我们会得到一个稍微好点的模型，因此验证误差会减小一点，但不会太多。最后，当更多的数据加入训练，训练误差会增加一点，验证误差会减小一点，它们会越来越靠近。\n图的中间一个刚刚好的模型（对应二次曲线模型）。跟前面描述的情况一样，随着参与训练的样本增加，训练误差会增加，验证误差会减小，最后它们会越来越接近，不同于左侧的情况，验证误差和训练误差会收敛到比较低的位置。\n图的右侧是一个高方差的模型（对应高阶拟合模型），一开始的情况跟前面两个模型的情况一样，但随着参与训练的样本增加，训练误差会增加，但会比较小，因为此时的模型试图对训练集进行记忆，同时验证误差会减小，但会比较大。最后，验证误差和训练误差会收敛，但不会靠近。\n接下来，我们看一个具体的例子：\n样本包含两类数据，如下所示：\n   Scatter Plot of data   针对该数据，我们拟合以下三个模型，看看它们的学习曲线如何：\n 逻辑回归模型 决策树模型 支持向量机模型  import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import learning_curve from sklearn.linear_model import LogisticRegression from sklearn.ensemble import GradientBoostingClassifier from sklearn.svm import SVC # Load data df = pd.read_csv('data.csv') X = np.array(df[['x1', 'x2']]) y = np.array(df['y']) # Fix random seed np.random.seed(55) def randomize(X, Y): permutation = np.random.permutation(Y.shape[0]) X2 = X[permutation,:] Y2 = Y[permutation] return X2, Y2 X2, y2 = randomize(X, y) # Construct three models to draw learning curves estimators = {} # Logistic Regression estimators['LR'] = LogisticRegression() # Decision Tree estimators['DT'] = GradientBoostingClassifier() # Support Vector Machine estimators['SVC'] = SVC(kernel='rbf', gamma=1000) plt.figure(figsize=(12, 6)) for i, m in enumerate(estimators): train_sizes, train_scores, test_scores = learning_curve( estimators[m], X2, y2, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 3)) train_scores_mean = np.mean(train_scores, axis=1) # train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) # test_scores_std = np.std(test_scores, axis=1) plt.subplot(1, 3, i + 1) plt.grid() plt.title(\u0026quot;Learning Curves of {}\u0026quot;.format(m)) plt.xlabel(\u0026quot;Training examples\u0026quot;) plt.ylabel(\u0026quot;Score\u0026quot;) plt.plot(train_scores_mean, 'o-', color=\u0026quot;g\u0026quot;, label=\u0026quot;Training score\u0026quot;) plt.plot(test_scores_mean, 'o-', color=\u0026quot;y\u0026quot;, label=\u0026quot;Cross-validation score\u0026quot;) plt.legend(loc=\u0026quot;best\u0026quot;) plt.tight_layout() plt.show()  以上代码将生成三个模型的学习曲线，我们来看一看：\n   这里我们要注意的一个地方是图中使用了得分而不是误差，得分跟误差恰好相反，所以在看图时我们要在脑海里试着将其倒过来。\n根据前面讲的不难分析出，决策树模型刚刚好，逻辑回归模型欠拟合，而支持向量机模型过拟合。 比如对支持向量机模型而言，模型在训练集上一直具有很高的得分（接近1），这意味着模型能够很好的拟合训练集，因此具有很小的训练误差，但在验证集上，得分一开始就不高，接着是减小，后趋于不变，这意味验证误差从一个较小值变大，然后收敛。训练得分和验证得分最后收敛，但不靠近。\n  这里的测试实为验证，不同于最终的测试，后者指模型评估，前者为模型验证。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1584098046,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1584098046,"objectID":"389262a3389827e74e796204507c9f54","permalink":"https://keris.github.io/zh/post/model-selection/","publishdate":"2020-03-13T19:14:06+08:00","relpermalink":"/zh/post/model-selection/","section":"post","summary":"如何选择一个好的机器模型，欠拟合和过拟合判断","tags":[],"title":"模型选择","type":"post"},{"authors":[],"categories":["机器学习"],"content":"目录  混淆矩阵 Accuracy Precision Recall F1 score $F_\\beta$ score ROC Curve AUC  回归指标     混淆矩阵     guessed positive guessed negative     positive true positives, #TP false negatives, #FN   negative false positives, #FP true negatives, #TN    假设某个模型在一组样本上的表现如下：\n   其中，蓝色的点为正例（positives)，橙色的点为负例（negatives），则混淆矩阵如下：\n    guessed positive guessed negative     positive 6 1   negative 2 5    Accuracy Accuracy，也就是准确率，表示样本中分类正确所占的比例。\n$$\\text{Accuracy} = \\frac{\\text{#TP} + \\text{#TN}}{\\text{#TP} + \\text{#FP} + \\text{#TN} + \\text{#FN}}$$\n其中:\n #TP表示true positive的数目 #FP表示false positive的数目 #TN表示true negative的数目 #FN表示false negative的数目  对于图1中的模型，准确率计算如下：\n$$\\text{Accuracy} = \\frac{6 + 5}{6 + 1 + 5 + 2} = \\frac{11}{14}= 78.57\\%$$\n准确率在数据偏斜的情况下将不再适用。比如在下图所示的例子中：\n   模型需要找出良好的交易，但我们的样本中绝大部分都是良好的交易，这导致无论我们的模型如何，它都具有很高的准确率，如图中所示为99.83%！，此时准确率将不能够评估我们的模型。\nPrecision Precision即精度，表示在所有预测为正例的样本中有多少是真正例。\n$$\\text{Precision} = \\frac{\\text{#TP}}{\\text{#TP} + \\text{#FP}}$$\n对于图1中的模型，精度计算如下：\n$$\\text{Precision} = \\frac{6}{6 + 2} = \\frac{6}{8} = 75\\%$$\nRecall Recall即为召回率，所有的正例样本中真正例所占的比例：\n$$\\text{Recall} = \\frac{\\text{#TP}}{\\text{#TP} + \\text{#FN}}$$\n对于图1中的模型，召回率计算如下：\n$$\\text{Recall} = \\frac{6}{6 + 1} = \\frac{6}{7} = 85.71\\%$$\nF1 score F1-score定义如下：\n$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n可以看到F1-score综合了精度和召回率，为精度和召回率的调和平均：在召回率不变的条件下，精度越高，F1-score越大；在精度不变的条件下，召回率越高，F1-score越大。\n$F_\\beta$ score $F_\\beta-\\text{score}$定义为：\n$$F_\\beta = (1 + \\beta^2) \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\beta^2 \\times \\text{Precision} + \\text{Recall}}$$\n可以看出来，当$\\beta = 1$时即为F1 score。\n下面我们看下$F_\\beta$随$\\beta$变化的情况：\n 当$\\beta = 0$时，通过计算不难得出$F_0 = \\text{Precision}$; 相反，当$\\beta$取很大的值时，$F_\\beta$将趋近召回率。  小结：当$\\beta$取较小的值时，精度比召回率重要，如$F_{0.5}$ score；当$\\beta$取较大值时召回率比精度重要，如$F_2$ score。\nROC Curve ROC Curve的全称为Receiver Operating Characteristic Curve，即受试者工作特性缺陷，它刻画了模型在所有分类阈值下的表现。\n要绘制该曲线，我们需要计算以下两个参数：\n True Positive Rate, TPR False Positive Rate, FPR  TPR，表示所有的正例样本中真正例的占比，不难发现，这其实跟召回率一个概念：\n$$\\text{TPR} = \\frac{\\text{#TP}}{\\text{#TP} + \\text{#FN}}$$\nFPR，表示所有的负例样本中假正例的占比：\n$$\\text{FPR} = \\frac{\\text{#FP}}{\\text{#FP} + \\text{#TN}}$$\n在不同的分类阈值下绘制TPR vs. FPR便得到ROC曲线：\n   AUC AUC的全称为Area Under a ROC Curve，即ROC曲线下的面积。\n   回归指标  Mean Absolute Error (MAE)  MAE即平均绝对误差，假设我们有一组样本${x_i, y_i}, i = 1, 2, \\cdots, n$，模型的输出为$\\text{preds} = [p_1, p_2, \\cdots, p_n]$，则MAE计算如下：\n$$\\text{MAE} = \\frac{1}{n}\\sum_i^n |y_i - p_i|$$\n借助sklearn，我们可以很方便地计算MAE：\nfrom sklearn.metrics import mean_absolute_error from sklearn.linear_model import LinearRegression clf = LinearRegression() preds = clf.fit(X, y) error = mean_absolute_error(y, preds)  从公式来看，MAE不可导，使得不能采用梯度下降方法进行优化，此时我们可以借助均方误差。\nMean Squared Error (MSE)  MSE即均方误差，定义如下：\n$$\\text{MSE} = \\frac{1}{n}\\sum_i^n (y_i - p_i)^2$$\n同样地，MSE通过sklearn可以很方便地进行计算：\nfrom sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression clf = LinearRegression() preds = clf.fit(X, y) error = mean_squared_error(y, preds)  R2 Score  R2分数通过将现有的模型与最简单的可能模型相比得出。\n比如，我们要拟合一堆数据点，那最简单的可能模型是什么？那就是我们取所有值的平均值，然后画一条水平线。此时我们可以计算出这个模型的均方误差大于线性回归模型的均方误差，如下图所示：\n   R2分数定义为：\n$$\\text{R2} = 1 - \\frac{\\text{MSE of 当前模型}}{\\text{MSE of 最简单的可能模型}}$$\n以上图为例，分子为线性回归模型（右侧）的均方误差，分母为简单模型（左侧）的均方误差。\n由于最简单的可能模型的均方误差大于线性回归模型的均方误差，我们在相比之后得到一个0到1之间的数字，从而R2得分也是一个0到1之间的数，并且有以下结论：\n R2越接近0，我们的模型越接近最简单的可能模型，此时模型是一个bad model 相反，R2越接近1，说明我们模型相比最简单的可能模型具有更小的均方误差，此时模型是一个good model  在sklearn中，我们如下计算R2得分：\n\u0026gt;\u0026gt;\u0026gt; from sklearn.metrics import r2_score \u0026gt;\u0026gt;\u0026gt; y_true = [1, 2, 4] \u0026gt;\u0026gt;\u0026gt; y_pred = [1.3, 2.5, 3.7] \u0026gt;\u0026gt;\u0026gt; r2_score(y_true, y_pred) \u0026gt;\u0026gt;\u0026gt; 0.9078571428571429  ","date":1583925611,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1583925611,"objectID":"9b5a8c97e2b1a3c38fd669a38e165a5e","permalink":"https://keris.github.io/zh/post/metrics/","publishdate":"2020-03-11T19:20:11+08:00","relpermalink":"/zh/post/metrics/","section":"post","summary":"了解机器学习模型的评估指标","tags":["评估指标"],"title":"评估指标","type":"post"},{"authors":[],"categories":["金融风控"],"content":"目录  信用评分卡模型  评分卡模型划分 如何利用评分卡对用户进行评分？ 小结   评分卡模型的开发  评分卡模型开发流程  数据获取 EDA 数据预处理 数据清洗 变量分箱  无监督分箱 有监督分箱 小结         信用评分卡模型 信用评分卡模型是一种最常见的金融风控手段。它是指根据客户的各种属性和行为数据构建一个信用评分模型，对客户进行信用评分，并据此决定是否给予授信以及授信的额度和利率，从而识别和减少在金融交易中存在的交易风险。\n以下为评分卡模型的示意图：\n  Credit Score Card Demo   评分卡模型划分 在不同的业务阶段我们可以构建不同的评分卡模型。根据借贷时间，评分卡模型可以划分为以下三种：\n 贷前：申请评分卡（Application score card），又称为A卡 贷中：行为评分卡（Behavior score card），又称为B卡 贷后：催收评分卡（Collection score card），又称为C卡  如何利用评分卡对用户进行评分？ 一个用户的评分等于基准分加上对客户各个属性的评分。对前面给出的示意图中的例子而言：\n客户评分 = 基准分 + 年龄评分 + 性别评分 + 婚姻状况评分 + 学历评分 + 月收入评分  现有一客户，其年龄为27岁，性别男，已婚，本科学历，月收入为10k，那么他的评分为：\n223(基准分) + 8(年龄评分) + 4(性别评分) + 8(婚姻状况评分) + 8(学历评分) + 13(月收入评分) = 264  小结 以上就是评分卡模型的具体用法。在前面的例子中，不难发现以下三个问题：\n 如何选择用户的属性？ 评分卡模型采用的是对属性的分段进行评分，那么如何进行有效的分段？ 如何对每个分段进行评分？  评分卡模型的开发 评分卡模型开发流程 信用评分卡模型的开发一般包括数据获取，EDA，数据预处理，变量筛选，模型开发及评估，生成评分卡模型，模型上线及监测。典型的开发流程图如下：\n  Credit Score Card Flowchart   数据获取 数据主要有两个来源：\n 金融机构自有数据：如用户的年龄，户籍，性别，收入，负债比，在本机构的借款和还款行为等 第三方数据：如用户在其他机构的借贷行为，用户的消费行为数据等  EDA EDA为Exploratory Data Analysis的简写，即探索性数据分析。这个阶段旨在了解数据的主要特性，如字段的缺失情况，异常值情况，中位数，分布等。最后制定一个合理的数据预处理方案。\n数据预处理 数据预处理主要包括数据清洗，变量分箱和WOE编码。\n数据清洗 数据清洗主要是对数据中的缺失值和异常值进行处理。一般我们选择删除缺失率超过某个阈值1（如30%，50%等）的变量。\n变量分箱 所谓的分箱定义如下：\n 对连续变量进行分段以离散化 将离散变量的多个状态进行合并，减少离散变量的状态数  通过分箱操作，我们达到了对变量分段的目的。\n常见的分箱类型如下：\n  常见的变量分箱类型   无监督分箱 无监督分箱主要包括三类：\n 等频分箱：把自变量的值按从小到大排序，将自变量的取值个数等分为k部分，每部分作为一个分箱 等距分箱：把自变量的值按从小到大排序，将自变量的取值范围分为k个等距的区间，每个区间作为一个分箱 聚类分箱：用k-means等聚类法将自变量分为k类，但需要在聚类过程中保证分箱的有序性  小结： 无监督分箱仅考虑了各个变量自身的结构，并没有考虑自变量与目标变量之间的关系，因此这种分箱方法不一定会带来模型性能的提升。\n有监督分箱 有监督分箱主要包括Split分箱和Merge分箱。\n Split分箱是一种自上而下的分箱方法，其和决策树比较相似，切分点的选择指标主要有entropy， gini指数和IV值等。 Merge分箱是一种自下而上的分箱方法，常见的merge分箱为ChiMerge分箱。  ChiMerge分箱的基本思想是：如果两个相邻区间具有相似的类分布，则合并它们，否则保持不变。ChiMerge通常采用卡方值来度量两相邻区间的类分布的相似性。\n计算卡方值的公式如下：\n$$ \\chi^2 = \\sum_{i=1}^2 \\sum_{j=1}^k \\frac{(A_{ij} - E_{ij})^2}{E_{ij}} $$\n其中：\n$k =$ 类别数目\n$A_{ij} =$ 第 $i$ 个区间中第$j$个类别的样本数目\n$E_{ij} = A_{ij}$的期望频率$=\\frac{R_i \\times C_j}{N}$，$R_i =$第$i$区间的样本数目$= \\sum_{j=1}^k A_{ij}$，$C_j = $第$j$个类别的样本数目$= \\sum_{i=1}^2 A_{ij}$\nChiMerge算法包含一个初始化步和一个自底向上的合并过程。\n 初始化：将需要离散化的属性的值按从小到大排序，将每个样本作为一个区间 合并区间  计算每对相邻区间的$\\chi^2$ 合并具有最小$\\chi^2$的相邻区间   如果所有相邻区间对的$\\chi^2$超过了$\\chi^2-threshold$，则停止，否则回到第2步  如何确定$\\chi^2-threshold$？为此我们需要选择一个想要的显著性水平和自有度，然后通过查表或者公式获得对应的$\\chi^2$。其中自由度为类别数目 - 1，例如，当有3个类别时，自由度为2。\n除了使用$\\chi^2-threshold$，还可以指定最小区间数和最大区间数来确保不会产生太少或者太多的区间。\n在金融风控中，当我们应用ChiMerge时，在初始化时往往有以下操作：\n 连续值按升序排列，离散值首先转化为坏客户的占比，然后再按升序排列 为了减少计算量，对于状态数大于某一阈值（建议为100）的变量，利用等频分箱进行粗分箱 若有缺失值，则缺失值单独作为一个分箱  在分箱完后还会做进一步的处理：\n 对于坏客户占比为0或者1的分箱进行合并（一个分箱内不能全为好客户或者坏客户） 对于样本占比超过95%的箱子进行删除 检查缺失分箱的坏客户比例是否和非缺失分箱相等，如果相等，进行合并  小结  分箱可以有效处理缺失值和异常值 分箱后数据和模型会更稳定 分箱可以简化逻辑回归模型，降低过拟合风险，提高泛化能力 分箱将特征统一变换为类别型变量 分箱后变量才可以使用标准的评分卡格式    阈值的选择需要根据实际情况确定。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1583227688,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1583227688,"objectID":"1cee129b977dc9746afbf84680375dc3","permalink":"https://keris.github.io/zh/post/credit-score-card/","publishdate":"2020-03-03T17:28:08+08:00","relpermalink":"/zh/post/credit-score-card/","section":"post","summary":"通过此文了信用评分卡模型的方方面面","tags":["评分卡"],"title":"信用评分卡模型","type":"post"},{"authors":[],"categories":[],"content":"RFM是一种用于分析客户价值的方法，常用于营销。其中RFM代表三个维度：\n Recency 表示最近一次客户购买的时间 Frequency 表示在统计周期内客户购买的次数 Monetary Value 表示统计周期内客户消费的总金额  接下来，我们使用RFM模型分析一个真实的在线购物数据。\n# Import libraries import pandas as pd import matplotlib.pyplot as plt import squarify from datetime import timedelta import seaborn as sns  # Read dataset online = pd.read_csv('data.csv', encoding='ISO-8859-1') online.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country     0 536365 85123A WHITE HANGING HEART T-LIGHT HOLDER 6 12/1/2010 8:26 2.55 17850.0 United Kingdom   1 536365 71053 WHITE METAL LANTERN 6 12/1/2010 8:26 3.39 17850.0 United Kingdom   2 536365 84406B CREAM CUPID HEARTS COAT HANGER 8 12/1/2010 8:26 2.75 17850.0 United Kingdom   3 536365 84029G KNITTED UNION FLAG HOT WATER BOTTLE 6 12/1/2010 8:26 3.39 17850.0 United Kingdom   4 536365 84029E RED WOOLLY HOTTIE WHITE HEART. 6 12/1/2010 8:26 3.39 17850.0 United Kingdom     online.dtypes  InvoiceNo object StockCode object Description object Quantity int64 InvoiceDate object UnitPrice float64 CustomerID float64 Country object dtype: object  # Convert InvoiceDate from object to datetime format online['InvoiceDate'] = pd.to_datetime(online['InvoiceDate'])  online.dtypes  InvoiceNo object StockCode object Description object Quantity int64 InvoiceDate datetime64[ns] UnitPrice float64 CustomerID float64 Country object dtype: object  # Check how many rows and columns online.shape  (541909, 8)  # Count transactions that don't have a customer id print('{:,} transactions don\\'t have a customer id' .format(online[online.CustomerID.isnull()].shape[0]))  135,080 transactions don't have a customer id  # Check invoice date range print('Transactions timeframe from {} to {}' .format(online.InvoiceDate.min(), online.InvoiceDate.max()))  Transactions timeframe from 2010-12-01 08:26:00 to 2011-12-09 12:50:00  # Drop NA values from online online.dropna(inplace=True)  # Group data by CustomerID # Create TotalSum column for online dataset online['TotalSum'] = online['Quantity'] * online['UnitPrice'] # Create snapshot date snapshot_date = online.InvoiceDate.max() + timedelta(days=1) print(snapshot_date) # Group by CustomerID data = online.groupby(['CustomerID']).agg({ 'InvoiceDate': lambda x: (snapshot_date - x.max()).days, 'InvoiceNo': 'count', 'TotalSum': 'sum' }) data.rename(columns={'InvoiceDate': 'Recency', 'InvoiceNo': 'Frequency', 'TotalSum': 'MonetaryValue'}, inplace=True)  2011-12-10 12:50:00  # Peek first 5 rows data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue   CustomerID        12346.0 326 2 0.00   12347.0 2 182 4310.00   12348.0 75 31 1797.24   12349.0 19 73 1757.55   12350.0 310 17 334.40     # Check how many rows and columns data.shape  (4372, 3)  在这里我们可以看到，根据CutomerID进行分组后，我们聚合生成了每个客户最近一次购买的时间，购买频率和消费额度，共4372条记录。接下来，我们需要对这三个维度进行打分，这可以通过.qcut()来进行。但在此之前，我们先看一看特征的分布情况。\n# Plot RFM distributions plt.figure(figsize=(12,10)) # Plot distribution of R plt.subplot(3, 1, 1) sns.distplot(data['Recency']) # Plot distribution of F plt.subplot(3, 1, 2) sns.distplot(data['Frequency']) # Plot distribution of M plt.subplot(3, 1, 3) sns.distplot(data['MonetaryValue'])  \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x1199779b0\u0026gt;  # Calculate R and F groups # Create labels for Recency and Frequency r_labels = range(4, 0, -1) f_labels = range(1, 5) # Assign these labels to 4 equal percentil groups r_groups = pd.qcut(data['Recency'], q=4, labels=r_labels) f_groups = pd.qcut(data['Frequency'], q=4, labels=f_labels) # Create new columns R and F data = data.assign(R=r_groups.values, F=f_groups.values) data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F   CustomerID          12346.0 326 2 0.00 1 1   12347.0 2 182 4310.00 4 4   12348.0 75 31 1797.24 2 2   12349.0 19 73 1757.55 3 3   12350.0 310 17 334.40 1 1     # Create labels form MonetaryValue m_labels = range(1, 5) m_groups = pd.qcut(data['MonetaryValue'], q=4, labels=m_labels) data = data.assign(M=m_groups.values)  data.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F M   CustomerID           12346.0 326 2 0.00 1 1 1   12347.0 2 182 4310.00 4 4 4   12348.0 75 31 1797.24 2 2 4   12349.0 19 73 1757.55 3 3 4   12350.0 310 17 334.40 1 1 2     # Concat RFM quartile values to create RFM segments def join_rfm(x): return str(x['R']) + str(x['F']) + str(x['M']) data['RFM_segment_concat'] = data.apply(join_rfm, axis=1) rfm = data rfm.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F M RFM_segment_concat   CustomerID            12346.0 326 2 0.00 1 1 1 111   12347.0 2 182 4310.00 4 4 4 444   12348.0 75 31 1797.24 2 2 4 224   12349.0 19 73 1757.55 3 3 4 334   12350.0 310 17 334.40 1 1 2 112     # Count num of unique segments rfm_count_unique = rfm.groupby('RFM_segment_concat')['RFM_segment_concat'].nunique() print(rfm_count_unique.sum())  62  以上结果显示，将RFM连接在一起这种方法共形成62个划分，但划分太多了不能用于实际应用。下面我们尝试将这三个值相加。\n# Calculate RFM_Score rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1) print(rfm['RFM_Score'].head())  CustomerID 12346.0 3.0 12347.0 12.0 12348.0 8.0 12349.0 10.0 12350.0 4.0 Name: RFM_Score, dtype: float64  # Define rfm_level function def rfm_level(df): if df['RFM_Score'] \u0026gt;= 9: return 'Can\\'t Loose Them' elif ((df['RFM_Score'] \u0026gt;= 8) and (df['RFM_Score'] \u0026lt; 9)): return 'Champions' elif ((df['RFM_Score'] \u0026gt;= 7) and (df['RFM_Score'] \u0026lt; 8)): return 'Loyal' elif ((df['RFM_Score'] \u0026gt;= 6) and (df['RFM_Score'] \u0026lt; 7)): return 'Potential' elif ((df['RFM_Score'] \u0026gt;= 5) and (df['RFM_Score'] \u0026lt; 6)): return 'Promising' elif ((df['RFM_Score'] \u0026gt;= 4) and (df['RFM_Score'] \u0026lt; 5)): return 'Needs Attention' else: return 'Require Activation' # Create a new variable RFM_Level rfm['RFM_Level'] = rfm.apply(rfm_level, axis=1) rfm.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  Recency Frequency MonetaryValue R F M RFM_segment_concat RFM_Score RFM_Level   CustomerID              12346.0 326 2 0.00 1 1 1 111 3.0 Require Activation   12347.0 2 182 4310.00 4 4 4 444 12.0 Can't Loose Them   12348.0 75 31 1797.24 2 2 4 224 8.0 Champions   12349.0 19 73 1757.55 3 3 4 334 10.0 Can't Loose Them   12350.0 310 17 334.40 1 1 2 112 4.0 Needs Attention     # Calculate average values for each RFM_Level, and return a size of each segment rfm_level_agg = rfm.groupby('RFM_Level').agg({ 'Recency': 'mean', 'Frequency': 'mean', 'MonetaryValue': ['mean', 'count'] }).round(1) # Print the aggregate dataset print(rfm_level_agg)   Recency Frequency MonetaryValue mean mean mean count RFM_Level Can't Loose Them 25.2 195.1 4130.3 1690 Champions 62.7 57.0 974.7 467 Loyal 78.8 39.7 724.2 447 Needs Attention 174.5 13.9 227.1 391 Potential 94.3 28.5 491.8 468 Promising 153.0 21.2 346.8 517 Require Activation 264.8 7.8 109.1 392  从以上结果我们可以看到，约60%的客户属于优质客户（前三类用户）。\n# Visualize segments rfm_level_agg.columns = rfm_level_agg.columns.droplevel() rfm_level_agg.columns = ['RecencyMean', 'FrequencyMean', 'MonetaryMean', 'Count']  # Create our plot and resize it fig = plt.gcf() ax = fig.add_subplot() fig.set_size_inches(16, 9) squarify.plot(sizes=rfm_level_agg['Count'], label=list(rfm_level_agg.index), alpha=.6) plt.title('RFM Segments', fontsize=18, fontweight='bold') plt.axis('off') plt.show()  ","date":1582702890,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1582702890,"objectID":"ef833db58e604d147197982b1096f887","permalink":"https://keris.github.io/zh/post/what-is-rfm-model/","publishdate":"2020-02-26T15:41:30+08:00","relpermalink":"/zh/post/what-is-rfm-model/","section":"post","summary":"简要说明RFM模型并给出一个实际的使用例子","tags":["RFM"],"title":"RFM模型及实践","type":"post"},{"authors":["杜利强"],"categories":["机器学习"],"content":"目录  术语  Logistic function Odds Logit function   逻辑回归  代价函数求导     术语 逻辑回归涉及到以下术语：\n Logistic function Odds Logit  Logistic function 逻辑回归中的 Logistic 正是出于 Logistic function ，它是一种 sigmoid function ，其接受任意实值，输出一个0到1之间的值。 标准的 logistic funtion 定义如下：\n$$\\sigma(z) = \\frac{e^z}{e^z + 1} = \\frac{1}{1 + e^{-z}}$$\n如下是它在区间$[-6, 6]$之间的图像：\n\n在逻辑回归中，我们使用对数几率（log odds），并假定它是输入特征的线性组合：\n$$z = \\ln \\frac{p(x)}{1 - p(x)}= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 = \\theta^T \\cdot x$$\n由上式我们可以得到，\n$$p(x) = \\sigma(z) = \\frac{1}{1 + e^{-\\theta^T \\cdot x}}$$\n在逻辑回归模型中，这里的 $p(x)$ 为因变量在成功情形下的概率，即 $p(y=1 \\mid x)$。\nOdds 如果 $p$ 表示一个事件发生的概率，那么odds定义为\n$$\\text{odds} = \\frac{p}{1 - p}$$ 也就是说，odds为发生的概率除以不发生的概率，亦可以说为成功的概率除以失败的概率。\nLogit function Logit为Log odds, logit function 定义为 logistic function的逆，即 $g = \\sigma^{-1}$。显而易见，我们有\n$$g(p(x)) = \\sigma_{-1}(p(x)) = \\text{logit}\\,p(x) = \\ln(\\frac{p(x)}{1 - p(x)}) = \\theta^T \\cdot x$$\n逻辑回归 逻辑回归是一个重要的机器学习算法，其目标是基于给定的数据$x$输出随机变量$y$为0或1的概率。\n考虑由$\\theta$参数化的线性模型，\n$$h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T \\cdot x}} = \\text{Pr}(y = 1 \\mid x;\\theta)$$\n从而，$\\text{Pr}(y=0 \\mid x;\\theta) = 1 - h_\\theta(x)$。\n因为$y \\in \\{0, 1 \\}$，我们有\n$$\\text{Pr}(y \\mid x;\\theta) = h_\\theta(x)^y (1 - h_\\theta(x))^{1 - y} $$\n似然函数为\n$$\\begin{aligned} L(\\theta \\mid x) \u0026amp;= \\Pr(Y\\mid X;\\theta) \\\\\n\u0026amp;= \\prod_i \\Pr(y^{(i)} \\mid x^{(i)};\\theta) \\\\\n\u0026amp;= \\prod_i h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{1-y^{(i)}} \\end{aligned}$$\n一般地，我们最大化对数似然函数，\n$$\\log L(\\theta \\mid x) = \\sum_{i=1}^{m}\\log \\Pr(y^{(i)} \\mid x^{(i)};\\theta)$$\n定义代价函数如下：\n$$J(\\theta) = -\\frac{1}{m} \\log L(\\theta \\mid x) = -\\frac{1}{m} \\sum_i^m (y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)})(1 - h_\\theta(x^{(i)})))$$\n容易看到，我们最大化对数似然函数就是最小化代价函数 $J(\\theta)$。在机器学习中，我们使用梯度下降最小化代价函数。\n代价函数求导 下面我们对代价函数$J(\\theta)$对$\\theta$进行求导，我们先考虑在一个样本上的代价函数\n$$J_1(\\theta) = -y \\log h_\\theta(x) - (1 - y)(1 - h_\\theta(x))$$\n现在对$J_1(\\theta)$对$\\theta$进行求导：\n注意到$\\frac{d\\sigma(z)}{dz} = \\sigma(z) (1 - \\sigma(z))$, 我们有\n$$\\begin{aligned}\\frac{\\partial}{\\partial \\theta_j} J_1(\\theta) \u0026amp;= -y \\frac{1}{h_\\theta(x)} h_\\theta(x) (1 - h_\\theta(x)) x_j - (1 - y) \\frac{1}{1 - h_\\theta(x)} (-1) h_\\theta(x) (1 - h_\\theta(x)) x_j \\\\ \u0026amp;= (h_\\theta(x) - y) x_j\\end{aligned} $$\n有了以上结果，我们有\n$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m}\\sum_i^m [h_\\theta(x^{(i)}) - y^{(i)}]x_j^{(i)}$$\n","date":1578991642,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1579496437,"objectID":"aebccbb495f02050f938f27e4bd41ef9","permalink":"https://keris.github.io/zh/post/logistic-regression/","publishdate":"2020-01-14T16:47:22+08:00","relpermalink":"/zh/post/logistic-regression/","section":"post","summary":"理解逻辑回归中涉及的术语以及从最大似然到最小化代价函数","tags":["logistic-regression","逻辑回归"],"title":"逻辑回归（Logistic Regression）","type":"post"},{"authors":[],"categories":[],"content":"在git的日常使用中更改提交的作者名和邮箱是一个十分常见的操作。例如，当你克隆了一个项目，如果你没有进行任何设置，此时提交的作者和邮件将使用全局选项，这可能不是你所要的。\n通过本文你将了解：\n 如何配置git使用的提交者用户名和邮箱 如何更改最近一次提交的作者名和邮件 批量更改提交的作者名和邮件  配置提交者用户名和邮箱 你有两种方式进行设置，一是全局配置，二是为每个repo单独配置。\n全局地更改提交者用户名和邮箱 使用git config并--global选项进行全局设置，如下所示：\n$ git config --global user.name \u0026quot;Du Liqiang\u0026quot; $ git config --global user.email \u0026quot;dlq137@gmail.com\u0026quot;  设置完毕后，后续的提交将使用以上提供的信息。\n为某个repo单独配置 全局选项可能并不适用于某个repo，此时就需要单独设置，使用git config但省略--global选项，如下：\n$ git config user.name \u0026quot;Du Liqiang\u0026quot; $ git config user.email \u0026quot;dlq137@gmail.com\u0026quot;  这里的设置将覆盖全局选项，并且只应用于当前的repo。\n更改最近一次提交的作者用户名和邮箱 如果你刚做了一次提交，发现用户名和邮箱并不是所要的，你可以使用--amend选项重新提交：\n$ git commit --amend --author=\u0026quot;Du Liqiang \u0026lt;dlq137@gmail.com\u0026gt;\u0026quot;  更改多次提交的作者用户名和邮箱 这个时候我们需要借助强大的rebase命令，首先我们找到上一次“好”的提交1，并假设其提交hash为0ad14fa5，执行：\n$ git rebase -i -p 0ad14fa5  此时我们会进入一个编辑器，将那些需要编辑的提交全标记为edit，接下来git会指导你完成每次提交的编辑，你需要做的就是执行：\n$ git commit --amend --author=\u0026quot;Du Liqiang \u0026lt;dlq137@gmail.com\u0026gt; --no-edit $ git rebase --continue  使用git filter-branch批量更改 除了以上交互式的更改方法，另一种方法是借助git的filter-branch命令，其允许你使用一个script批量处理大量的提交。\n如下命令筛选提交邮箱为WRONG_EMAIL的提交，并将其用户名和邮箱分别设置为NEW_NAME和NEW_EMAIL对应的值。\n$ git filter-branch --env-filter ' WRONG_EMAIL=\u0026quot;wrong@example.com\u0026quot; NEW_NAME=\u0026quot;New Name Value\u0026quot; NEW_EMAIL=\u0026quot;correct@example.com\u0026quot; if [ \u0026quot;$GIT_COMMITTER_EMAIL\u0026quot; = \u0026quot;$WRONG_EMAIL\u0026quot; ] then export GIT_COMMITTER_NAME=\u0026quot;$NEW_NAME\u0026quot; export GIT_COMMITTER_EMAIL=\u0026quot;$NEW_EMAIL\u0026quot; fi if [ \u0026quot;$GIT_AUTHOR_EMAIL\u0026quot; = \u0026quot;$WRONG_EMAIL\u0026quot; ] then export GIT_AUTHOR_NAME=\u0026quot;$NEW_NAME\u0026quot; export GIT_AUTHOR_EMAIL=\u0026quot;$NEW_EMAIL\u0026quot; fi ' --tag-name-filter cat -- --branches --tags  参考  https://www.git-tower.com/learn/git/faq/change-author-name-email    该次提交之前的提交具有正确的用户名和邮箱，其后的需要进行更改。 \u0026#x21a9;\u0026#xfe0e;\n  ","date":1573120043,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1573120043,"objectID":"9f6fd127ef1ad95f23bfd5dc1c97aaa2","permalink":"https://keris.github.io/zh/post/change-author-name-email/","publishdate":"2019-11-07T17:47:23+08:00","relpermalink":"/zh/post/change-author-name-email/","section":"post","summary":"在git的日常使用中更改提交的作者名和邮箱是一个十分常见的操作。例如，当你克隆了一个项目，如果你没有进行任何设置，此时提交的作者和邮件将使用全局选项，这可能不是你所要的。\n","tags":["git","change-author"],"title":"更改git提交中的作者和邮件信息","type":"post"}]
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>杜利强</title>
    <link>https://keris.github.io/zh/authors/%E6%9D%9C%E5%88%A9%E5%BC%BA/</link>
      <atom:link href="https://keris.github.io/zh/authors/%E6%9D%9C%E5%88%A9%E5%BC%BA/index.xml" rel="self" type="application/rss+xml" />
    <description>杜利强</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>zh-Hans</language><lastBuildDate>Sun, 05 Apr 2020 19:40:58 +0800</lastBuildDate>
    <image>
      <url>https://keris.github.io/images/icon_hu0fc6cda6d9ee8f97aed5fca718c40606_62996_512x512_fill_lanczos_center_2.png</url>
      <title>杜利强</title>
      <link>https://keris.github.io/zh/authors/%E6%9D%9C%E5%88%A9%E5%BC%BA/</link>
    </image>
    
    <item>
      <title>十元蛋炒饭</title>
      <link>https://keris.github.io/zh/post/shi-yuan-danchaofan/</link>
      <pubDate>Sun, 05 Apr 2020 19:40:58 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/shi-yuan-danchaofan/</guid>
      <description>&lt;p&gt;随着逐步的复工，附近的餐馆陆续开始营业了，这使得我可以选择走出屋子上大街吃上一顿。早上喝了一碗牛奶泡燕麦，吃了一片土司面包，些许坚果和四个鸡蛋。所以到吃午饭时也不怎么饿，由于养成了按时吃饭的习惯，就还是出去吃，但心里暗地提示自己少吃一些。小区马路对面有好几家餐馆，另有一家便利店。而另一侧只有一家，提供新疆美食。我先是去了便利店，环顾一圈未发现引起胃口的什物便出了门。隔壁餐馆门外贴着菜单，看过一番之后，蛋炒饭入了我眼，并且只卖10块钱。其实，这家餐馆所有的饭都比较价廉。&lt;/p&gt;
&lt;p&gt;过了不一会儿蛋炒饭便好了，一个年纪五十左右的阿姨端到了我这边。我用勺子吃了几口，整体味道还算不错，但有的地方会发现比较咸。这份蛋炒饭除了米饭和鸡蛋，包含了黄瓜、胡萝卜和洋葱，并且份量中等，这十分对得起10块钱的价格，我寻思老板是不是在做公益，还是别的家赚取的太多。&lt;/p&gt;
&lt;p&gt;说起蛋炒饭，我最爱吃家里做的，我妈炒的。在家里只有在剩了饭才有机会做炒饭，并且不加鸡蛋。剩饭会和剩菜一起炒，这样炒出来往往比较油腻，但丝毫不影响饭的抢手和好吃。后来我想了想，其实要天天这样吃肯定也会腻。之所以我一直觉得家里的炒饭具有特别的味道，一个是西北的饮食习惯让我们以面食为主，米饭为辅，所以吃自家炒饭的机会不多；二是剩饭一般较少，做出来的炒饭就比较少，对于一家四口而言也就每人一小碗。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>先手优势</title>
      <link>https://keris.github.io/zh/post/xianshou-throwing/</link>
      <pubDate>Tue, 24 Mar 2020 14:00:50 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/xianshou-throwing/</guid>
      <description>&lt;p&gt;棋经有云，“宁失一子，勿失一先”，可见先手的好处所在。为了建立对先手直观的理解，我们来玩一个游戏：&lt;/p&gt;
&lt;p&gt;&lt;em&gt;两个人玩投掷硬币的游戏，先掷出正面的人获胜，那么你是选择先掷还是后掷？&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;数学解法&#34;&gt;数学解法&lt;/h2&gt;
&lt;p&gt;假设硬币是公平的，即出现正面和反面的概率一样，均为&lt;code&gt;1/2&lt;/code&gt;。那么，在先手下获胜的情况是第&lt;code&gt;n&lt;/code&gt;次掷得正面，前&lt;code&gt;n-1&lt;/code&gt;次都为反面，其中&lt;code&gt;n&lt;/code&gt;为奇数。所以先手获胜的概率为&lt;/p&gt;
&lt;p&gt;$$p(\text{win}|\text{first}) = \sum_{k=0}^{\infty}(\frac{1}{2})^{2k + 1} = \frac{2}{3}$$&lt;/p&gt;
&lt;p&gt;相反，在后手下获胜的情况是第&lt;code&gt;n&lt;/code&gt;次掷得正面，前&lt;code&gt;n-1&lt;/code&gt;次均为反面，其中&lt;code&gt;n&lt;/code&gt;为偶数。所以，后手获胜的概率为&lt;/p&gt;
&lt;p&gt;$$p(\text{win}|\text{second}) = \sum_{k=1}^{\infty}(\frac{1}{2})^{2k} = \frac{1}{3}$$&lt;/p&gt;
&lt;p&gt;可以发现，先手获胜的概率为2/3，而后手获胜的概率为1/3，这告诉我们在生活中抓住先机，主动出击往往更能取得胜利。&lt;/p&gt;
&lt;h2 id=&#34;模拟法&#34;&gt;模拟法&lt;/h2&gt;
&lt;p&gt;前面我们用数学的方法进行了求解，下面来看看如何用程序来模拟，为此我写了一个Python程序：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# coding: utf-8
# filename: code.py
# Author: Liqiang Du &amp;lt;keris.du@gmail.com&amp;gt;
import random
import numpy as np


def simulate(n_games):
    &amp;quot;&amp;quot;&amp;quot;
    Simulate coin throwing game, the game is over when head appears.

    Args:
        n_games (int): the number of games.

    Returns:
        tuple: the first is the probability of win when first play,
        and the second the probability of win when second play.
    &amp;quot;&amp;quot;&amp;quot;
    win_count = np.array([0, 0])
    for i in range(n_games):
        game_over = False
        n_tails = 0
        while not game_over:
            x = random.randint(0, 1)  # 1 indicates head and 0 tail
            if x == 1:
                game_over = True
            else:
                n_tails += 1
        win_count[n_tails % 2] += 1
    return win_count.astype(float) / n_games


if __name__ == &amp;quot;__main__&amp;quot;:
    n_games = 100000
    probs = simulate(n_games)
    print(probs)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;程序的核心部分在于&lt;code&gt;simulate&lt;/code&gt;，每次游戏，出现正面游戏就结束，同时记录反面出现的次数，如果该次数为偶数，则先手为赢，否则后手为赢。最后程序返回一个元祖，第一个表示先手获胜的概率，第二个为后手获胜的概率。&lt;/p&gt;
&lt;p&gt;我们运行一下程序看看结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ python code.py
[0.66436 0.33564]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们模拟了10万次游戏，程序返回的结果跟数学求解结果十分接近。&lt;/p&gt;
&lt;p&gt;至此文章就完了。在这里我通过一个简单的游戏说明了先手的好处，这告诉我们一个道理，主动出击获得先手优势往往更能够成功。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>随机数生成</title>
      <link>https://keris.github.io/zh/post/random-number-generator/</link>
      <pubDate>Wed, 18 Mar 2020 11:45:54 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/random-number-generator/</guid>
      <description>&lt;p&gt;最近参加了一家公司的面试，其中一个问题是：“如何在C语言中生成一个真随机数？” 我当时想到的答案是使用&lt;code&gt;rand()&lt;/code&gt;，但&lt;code&gt;rand()&lt;/code&gt;返回的其实是一个伪随机数，值的范围是&lt;code&gt;[0, RAND_MAX]&lt;/code&gt;。其中，&lt;code&gt;RAND_MAX&lt;/code&gt;跟具体的实现有关，也就是说在不同的实现中可能有不同的值。&lt;/p&gt;
&lt;p&gt;以上便是这篇博客的背景，也可以看出自己对随机数生成（RNG）掌握的也不全面，所以试图通过此文弥补一下短板。&lt;/p&gt;
&lt;h2 id=&#34;随机数的应用&#34;&gt;随机数的应用&lt;/h2&gt;
&lt;p&gt;随机数的应用有很多方面，包括赌博，统计采样，计算机仿真，加密等。&lt;/p&gt;
&lt;h2 id=&#34;真随机数与伪随机数&#34;&gt;真随机数与伪随机数&lt;/h2&gt;
&lt;p&gt;简单来讲，真随机数无法预测，而伪随机数在知道种子值后便能复现。我们先来了解一下生成随机数的两种方法。&lt;/p&gt;
&lt;p&gt;在第一种方法中，我们测量某个被认为是随机的物理现象，然后补偿测量过程中可能的偏差。这种随机源包括空气噪声，热噪声以及其他外部的电磁和量子现象。要产生一个随机数，我们需要获取足够的墒（entropy），但获取速率取决于被测量的物理现象。因此，这种产生随机数的方法是阻塞的（blocking）的，速率受限的。举个例子，在大部分Linux发行版中，伪随机设备文件&lt;code&gt;/dev/random&lt;/code&gt;就会阻塞直到从环境获取到足够的墒。由于这种阻塞行为，从&lt;code&gt;/dev/random&lt;/code&gt;大批量读以便用随机位填充磁盘经常会很慢，就是因为墒源是阻塞型的。&lt;/p&gt;
&lt;p&gt;在第二种方法中，我们使用计算的方法来生成明显随机值组成的长序列，但实际上结果完全由一个较短的初始值确定，这个初始值也叫种子值。因此，整个看似随机的序列在知道种子值的情况下能够被复现。相比第一种方法，第二种方法是非阻塞（non-blocking）的，所以其生成速率不受外部事件的影响。&lt;/p&gt;
&lt;p&gt;在实际应用中，一些系统采取混合的办法，即在可用的时候采用自然源提供的随机性，否则使用周期性重新设置种子、软件基于的密码安全的伪随机数生成器（CSPRNGs）。&lt;/p&gt;
&lt;p&gt;在这里小结一下：真随机数依赖于收集自然发生的墒，有了足够的墒方可生成随机数，因此是阻塞的，速率有限的；伪随机数使用计算方法产生，在知道种子值后能够复现，非阻塞，速率不受限。&lt;/p&gt;
&lt;h2 id=&#34;c语言中的随机数生成&#34;&gt;C语言中的随机数生成&lt;/h2&gt;
&lt;p&gt;在C语言中，标准库&lt;code&gt;stdlib.h&lt;/code&gt;提供了两个函数&lt;code&gt;int rand(void)&lt;/code&gt;和&lt;code&gt;void srand(unsigned)&lt;/code&gt;来帮助我们生成伪随机数。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;rand()&lt;/code&gt;返回一个位于区间&lt;code&gt;[0, RAND_MAX]&lt;/code&gt;的整数，&lt;code&gt;RAND_MAX&lt;/code&gt;在不同的实现中可能不同，但被保证至少是&lt;code&gt;32767&lt;/code&gt;。在调用&lt;code&gt;rand()&lt;/code&gt;前，如果我们不设置种子，系统默认我们设置种子为1，即调用了&lt;code&gt;srand(1)&lt;/code&gt;，在这种情况下，我们每次运行将会产生同样的值，下面看一个例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;

int main() {
    int num = rand();
    printf(&amp;quot;Random number: %d\n&amp;quot;, num);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们编译以上代码并运行5次：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ gcc gen_rand.c &amp;amp;&amp;amp; for i in `seq 1 5`;do ./a.out;done
Random number: 16807
Random number: 16807
Random number: 16807
Random number: 16807
Random number: 16807
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到，程序在每次运行时均生成了同样的值，这是因为种子值均为1。&lt;/p&gt;
&lt;p&gt;接下来，我们再看看手动设置种子的情况：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;time.h&amp;gt;

int main() {
    srand((unsigned)time(0)); // use current time as seed
    int num = rand();
    printf(&amp;quot;Random number: %d\n&amp;quot;, num);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这一次，我们将种子设为当前时间。同样地，我们编译并运行5次：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ gcc gen_rand1.c &amp;amp;&amp;amp; for i in `seq 1 5`;do ./a.out;done
Random number: 7890298
Random number: 7890298
Random number: 7890298
Random number: 7890298
Random number: 7890298
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;结果看似跟前面没有不同，因为5次调用依然生成了同样的值。其实，这个差别很细微，我们将种子设为当前时间，但只有在重新编译时才能有不同的种子值。因此，两次编译之间生成的值应该不同：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ gcc gen_rand1.c &amp;amp;&amp;amp; ./a.out
Random number: 12713907
$ gcc gen_rand1.c &amp;amp;&amp;amp; ./a.out
Random number: 12747521
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;c中的随机数生成&#34;&gt;C++中的随机数生成&lt;/h2&gt;
&lt;p&gt;在C++中，我们仍然可以使用&lt;code&gt;rand()&lt;/code&gt;和&lt;code&gt;srand()&lt;/code&gt;来生成随机数，但这已经不是标准推荐的做法，相反我们应该使用&lt;code&gt;random&lt;/code&gt;库提供的工具类来生成随机数。&lt;/p&gt;
&lt;p&gt;下面给出C++版本的代码，运行结果跟C语言别无二致，不再赘述。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;cstdlib&amp;gt;
#include &amp;lt;ctime&amp;gt;

using namespace std;

int main() {
    // srand((unsigned)time(nullptr)); // use current time as seed
    int r = rand();
    cout &amp;lt;&amp;lt; &amp;quot;Random number between 0 and &amp;quot; &amp;lt;&amp;lt; RAND_MAX &amp;lt;&amp;lt; &amp;quot; :&amp;quot;;
    cout &amp;lt;&amp;lt; r &amp;lt;&amp;lt; &#39;\n&#39;;
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接下来，我们使用&lt;code&gt;random&lt;/code&gt;库提供的工具来生成随机数。&lt;code&gt;random&lt;/code&gt;库提供了两种类型的类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;均匀分布随机位生成器（URBGs），既包括伪随机数生成和真随机数生成（如果可用）&lt;/li&gt;
&lt;li&gt;随机数分布&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;详细的信息请参见 &lt;a href=&#34;https://en.cppreference.com/w/cpp/numeric/random&#34;&gt;https://en.cppreference.com/w/cpp/numeric/random&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;下面给出一个使用随机数引擎生成随机数的例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;random&amp;gt;

using namespace std;

int main() {
    random_device rd;
    default_random_engine e1(rd());
    cout &amp;lt;&amp;lt; e1() &amp;lt;&amp;lt; endl;
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们编译以上代码，并运行5次：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ g++ main.cc -std=c++17 &amp;amp;&amp;amp; for i in `seq 1 5`;do ./a.out;done
17429349
872858569
1540494345
40333012
95212997
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;从这里可以看到，我们编译代码一次，多次运行，每次会产生不同的值，序列变得不可预测。&lt;/p&gt;
&lt;p&gt;此外，我们可以将生成的随机数转换到某个闭区间的均匀分布或者正态分布，下面是一个均匀分布的例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;iomanip&amp;gt;
#include &amp;lt;string&amp;gt;
#include &amp;lt;random&amp;gt;
#include &amp;lt;map&amp;gt;

using namespace std;

int main() {
    random_device rd;
    default_random_engine e1(rd());
    uniform_int_distribution&amp;lt;int&amp;gt; dis(1, 6);
    cout &amp;lt;&amp;lt; dis.min() &amp;lt;&amp;lt; endl;
    cout &amp;lt;&amp;lt; dis.max() &amp;lt;&amp;lt; endl;
    map&amp;lt;int, int&amp;gt; hist;
    for (int i = 0; i != 10000; i++) {
        hist[dis(e1)]++;
    }
    for (auto[num, count] : hist) {
        cout &amp;lt;&amp;lt; num &amp;lt;&amp;lt; &#39; &#39; &amp;lt;&amp;lt; string(count / 200, &#39;*&#39;) &amp;lt;&amp;lt; &#39;\n&#39;;
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;编译以上代码并运行，结果如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ g++ -std=c++17 main.cc &amp;amp;&amp;amp; ./a.out
1
6
1 ********
2 ********
3 *******
4 ********
5 ********
6 ********
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们来分析一下。首先，我们输出均匀分布可能生成的最小值和最大值，由于我们考虑的是闭区间&lt;code&gt;[1, 6]&lt;/code&gt;上的均匀分布，显然最小值和最大值分别为1和6。然后我们将产生的随机数转换到区间上的某个值，并统计出现次数。最后，我们绘制直方图，从结果来看，每个值出现的次数基本一样多（我们进行了10000次实验！）。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>一个例子导向的对变参模版的理解</title>
      <link>https://keris.github.io/zh/post/variadic-template/</link>
      <pubDate>Tue, 17 Mar 2020 10:13:02 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/variadic-template/</guid>
      <description>&lt;p&gt;最近看C++编程语言第四版&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，阅读到线程这一章，作者在讲解mutex时给出了一个例子，自己照着写了一个发现跑不通，研究后发现对变参模版理解不够，便写了这篇文章。&lt;/p&gt;
&lt;p&gt;首先，我们看看书本上的例子：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;mutex cout_mutex;

template&amp;lt;typename Arg, typename... Args&amp;gt;
void write(Arg a, Args... tail) {
    cout_mutex.lock();
    cout &amp;lt;&amp;lt; a;
    write(tail...);
    cout_mutex.unlock();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个例子旨在说明deadlock，因为我们在&lt;code&gt;write&lt;/code&gt;函数中递归调用它，而该函数在进行输出前需要获取互斥变量。为了解除死锁，我们可以使用&lt;code&gt;recursive_mutex&lt;/code&gt;，这种类型的mutex允许递归地获取。&lt;/p&gt;
&lt;p&gt;基于以上想法，我写了一个例子来进行测试：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;mutex&amp;gt;

using namespace std;

recursive_mutex cout_mutex;

template&amp;lt;typename Arg, typename... Args&amp;gt;
void write(Arg a, Args... tail) {
    cout_mutex.lock();
    cout &amp;lt;&amp;lt; a;
    write(tail...);
    cout_mutex.unlock();
}

int main(int argc, char* argv[]) {
    write(&amp;quot;hello,&amp;quot;, &amp;quot;world&amp;quot;);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当我尝试编译以上代码却发现报出错误：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ g++ test.cc -std=c++17
test.cpp:12:6: error: no matching function for call to &#39;write&#39;
     write(tail...);
     ^~~~~
test.cpp:12:6: note: in instantiation of function template
      specialization &#39;write&amp;lt;const char *&amp;gt;&#39; requested here
test.cpp:17:6: note: in instantiation of function template
      specialization &#39;write&amp;lt;const char *, const char *&amp;gt;&#39; requested here
     write(&amp;quot;hello,&amp;quot;, &amp;quot;world&amp;quot;);
     ^
test.cpp:9:7: note: candidate function template not viable: requires at
      least argument &#39;a&#39;, but no arguments were provided
 void write(Arg a, Args... tail) {
      ^
1 error generated.
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;在编译时我使用了c++17标准。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可以看到，编译器报告了一个错误，&lt;strong&gt;对&amp;rsquo;write&#39;的调用没有匹配的函数&lt;/strong&gt;，这是为何？&lt;/p&gt;
&lt;p&gt;对于以上的实现，&lt;code&gt;write(&amp;quot;hello,&amp;quot;, &amp;quot;world&amp;quot;)&lt;/code&gt;可以拆分成：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;输出&lt;code&gt;hello,&lt;/code&gt;，此时tail包含一个参数即&lt;code&gt;world&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;输出&lt;code&gt;world&lt;/code&gt;，此时tail为空，但write函数至少需要一个参数，所以产生以上错误。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面我们增加一个接受空参数的write函数来验证：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;mutex&amp;gt;

using namespace std;

recursive_mutex cout_mutex;

void write() {
    cout &amp;lt;&amp;lt; &amp;quot;&amp;quot;; // output nothing
}

template&amp;lt;typename Arg, typename... Args&amp;gt;
void write(Arg a, Args... tail) {
    cout_mutex.lock();
    cout &amp;lt;&amp;lt; a;
    write(tail...);
    cout_mutex.unlock();
}


int main(int argc, char* argv[]) {
    write(&amp;quot;hello,&amp;quot;, &amp;quot;world&amp;quot;);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在进行上述操作后，代码编译通过，运行后输出以下结果：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;$ ./a.out
$ hello,world
&lt;/code&gt;&lt;/pre&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;英文名为：&lt;a href=&#34;https://www.amazon.com/dp/0321958322/ref=cm_sw_em_r_mt_dp_U_xP11DbJ5REYKZ&#34;&gt;The C++ Programming Language, 4th Edition&lt;/a&gt;. 作者为Bjarne Stroustrup. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>机器学习模型选择</title>
      <link>https://keris.github.io/zh/post/model-selection/</link>
      <pubDate>Fri, 13 Mar 2020 19:14:06 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/model-selection/</guid>
      <description>&lt;h2&gt;目录&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#两种类型的错误&#34;&gt;两种类型的错误&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#模型复杂图&#34;&gt;模型复杂图&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#k折交叉验证&#34;&gt;K折交叉验证&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#学习曲线&#34;&gt;学习曲线&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h3 id=&#34;两种类型的错误&#34;&gt;两种类型的错误&lt;/h3&gt;
&lt;p&gt;在选择模型时我们可能犯两种类型的错误，选择过于简单的模型或者过于复杂的模型，前者对应&lt;strong&gt;欠拟合&lt;/strong&gt;，后者对应&lt;strong&gt;过拟合&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;欠拟合发生时，模型具有高偏差（high bias），模型不能很好地拟合训练集；过拟合发生时，模型具有高方差（high variance），模型试图对训练数据进行记忆而不是学习其特点，在训练集上表现很好，但在测试集上表现很差。&lt;/p&gt;
&lt;p&gt;下面我们看一个欠拟合的例子：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;underfitting.png&#34; &gt;
&lt;img data-src=&#34;underfitting.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;在这个例子中，左边的模型使用一个二次曲线去拟合数据点，而右边则是使用直线去拟合，可以发现直线并不能很好地拟合数据，这就是欠拟合的情形。&lt;/p&gt;
&lt;p&gt;了解了欠拟合，我们来看一个过拟合的例子：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;overfitting.png&#34; data-caption=&#34;Overfitting&#34;&gt;
&lt;img data-src=&#34;overfitting.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Overfitting
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;在这个例子中，左边的模型使用一个二次曲线去拟合数据点，而右边的模型试图对训练集进行记忆，因而它在训练集上表现很好，但它没有学习到数据的良好属性以很好地泛化到测试集，这就是过拟合的情形。&lt;/p&gt;
&lt;p&gt;前面两个例子属于回归模型，在分类模型中我们也会看到欠拟合和过拟合。&lt;/p&gt;
&lt;p&gt;分类模型中的欠拟合：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;underfitting-classification.png&#34; data-caption=&#34;Underfitting in a Classification Model&#34;&gt;
&lt;img data-src=&#34;underfitting-classification.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Underfitting in a Classification Model
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;分类模型中的过拟合：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;overfitting-classification.png&#34; data-caption=&#34;Overfitting in a Classification Model&#34;&gt;
&lt;img data-src=&#34;overfitting-classification.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Overfitting in a Classification Model
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;最后，我们总结一下：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;tradeoff-high-bias-high-variance.png&#34; data-caption=&#34;Tradeoff between High Bias and High Variance&#34;&gt;
&lt;img data-src=&#34;tradeoff-high-bias-high-variance.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Tradeoff between High Bias and High Variance
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;模型复杂图&#34;&gt;模型复杂图&lt;/h3&gt;
&lt;p&gt;在前面一部分，我们学习了欠拟合和过拟合，在这一部分，我们了解一下模型复杂图，即评估指标随着模型复杂性的提升如何变化。&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;model-complexity-graph.png&#34; data-caption=&#34;Model Complexity Graph&#34;&gt;
&lt;img data-src=&#34;model-complexity-graph.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model Complexity Graph
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;在上面这个图中，左侧对应欠拟合的情形，此时模型在训练集和验证集上都具有较大的误差；右侧对应过拟合的情形，模型在训练集上具有较小的误差，而在验证集上具有较大的误差；圈起来的那个地方对应刚刚好的情形，此时模型在训练集和验证集上都具有较小的误差，过了这个点，在训练集上的误差持续减小，但在验证集上的误差开始不断变大。&lt;/p&gt;
&lt;p&gt;可能你注意到了一个新概念，&lt;strong&gt;验证集&lt;/strong&gt;，为什么不用测试集？&lt;/p&gt;
&lt;p&gt;在构建机器学习模型时，一个务必要坚持的原则是&lt;strong&gt;测试集只能用于最后的模型评估，而不能参与模型训练&lt;/strong&gt;。那么我们怎么选择一个好的模型，验证集即用于此目的。&lt;/p&gt;
&lt;p&gt;那怎么得到验证集？下图可以给你一个直观的理解：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;cross-validation.png&#34; data-caption=&#34;Cross Validation&#34;&gt;
&lt;img data-src=&#34;cross-validation.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Cross Validation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;可以看到，训练集中的一小部分被用来作为验证集，借助验证集我们评估模型是否欠拟合，过拟合或者刚刚好，即验证集用来进行决策，帮助我们选择一个好的模型。最后，测试集用于最终的模型评估。&lt;/p&gt;
&lt;h3 id=&#34;k折交叉验证&#34;&gt;K折交叉验证&lt;/h3&gt;
&lt;p&gt;可能你注意到了，由于从训练集中划出了一小部分作为验证集，这使得训练数据变少了，这会对模型产生不利因素，为此你需要使用K折交叉验证。&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;k-fold-1.png&#34; &gt;
&lt;img data-src=&#34;k-fold-1.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;在上面这个图中，我们有12个样本，其中实心的用于训练，空心的用于测试&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;。这是一个4折交叉验证，每次其中一折用于测试，剩余三折用于训练。第一次，第一折样本&lt;code&gt;[0,1,2]&lt;/code&gt;用于测试；第二次，第二折样本&lt;code&gt;[3,4,5]&lt;/code&gt;用于测试；第三次，第三折样本&lt;code&gt;[6,7,8]&lt;/code&gt;用于测试；最后一次，最后一折样本&lt;code&gt;[9,10,11]&lt;/code&gt;用于测试。 可以看到，经过4折交叉验证，&lt;strong&gt;训练集中的所有样本都参与了模型训练&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;接下来，我们看看如何在&lt;code&gt;sklearn&lt;/code&gt;中进行K折交叉验证：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from sklearn.model_selection import KFold
kf = KFold(4)
X = np.arange(12)
for train_indices, test_indices in kf.split(X):
    print(train_indices, test_indices)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以上代码产生如下结果：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[ 3  4  5  6  7  8  9 10 11] [0 1 2]
[ 0  1  2  6  7  8  9 10 11] [3 4 5]
[ 0  1  2  3  4  5  9 10 11] [6 7 8]
[0 1 2 3 4 5 6 7 8] [ 9 10 11]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到，输出结果跟图中的例子完全一致。可能你也注意到了，在这个例子中，每一折都是顺序产生，其实我们可以进行随机选取。&lt;/p&gt;
&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;k-fold-2.png&#34; &gt;
&lt;img data-src=&#34;k-fold-2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

同样地，我们看看在&lt;code&gt;sklearn&lt;/code&gt;中如何做：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
from sklearn.model_selection import KFold
kf = KFold(4, shuffle=True)
X = np.arange(12)
for train_indices, test_indices in kf.split(X):
    print(train_indices, test_indices)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以上代码的结果如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[0 1 2 3 4 6 7 8 9] [ 5 10 11]
[ 0  1  2  4  5  6  7 10 11] [3 8 9]
[ 0  1  3  4  5  8  9 10 11] [2 6 7]
[ 2  3  5  6  7  8  9 10 11] [0 1 4]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可以看到，每一折中的样本不再连续，而是随机抽取的。&lt;/p&gt;
&lt;h3 id=&#34;学习曲线&#34;&gt;学习曲线&lt;/h3&gt;
&lt;p&gt;在模型的训练过程中，通过绘制训练误差和验证误差 vs. 参与训练的样本数，我们可以得到两条曲线。&lt;/p&gt;
&lt;p&gt;下面我们来看一个图：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;learning-curves-1.png&#34; data-caption=&#34;Learning Curves&#34;&gt;
&lt;img data-src=&#34;learning-curves-1.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Learning Curves
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;图的左侧是一个高偏差的模型（&lt;em&gt;对应前述的线性模型&lt;/em&gt;），即模型欠拟合。在训练样本比较少时，模型能很好地拟合数据，因此训练误差很小，但当评估模型时，由于训练样本较少，模型的表现不会太好，因此验证误差会很大。随着参与训练的样本增加，更多的样本需要拟合，拟合难度增加，训练误差可能会增大一点。此时，由于使用了更多的训练数据，我们会得到一个稍微好点的模型，因此验证误差会减小一点，但不会太多。最后，当更多的数据加入训练，训练误差会增加一点，验证误差会减小一点，它们会越来越靠近。&lt;/p&gt;
&lt;p&gt;图的中间一个刚刚好的模型（&lt;em&gt;对应二次曲线模型&lt;/em&gt;）。跟前面描述的情况一样，随着参与训练的样本增加，训练误差会增加，验证误差会减小，最后它们会越来越接近，不同于左侧的情况，&lt;strong&gt;验证误差和训练误差会收敛到比较低的位置&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;图的右侧是一个高方差的模型（&lt;em&gt;对应高阶拟合模型&lt;/em&gt;），一开始的情况跟前面两个模型的情况一样，但随着参与训练的样本增加，训练误差会增加，但会比较小，因为此时的模型试图对训练集进行记忆，同时验证误差会减小，但会比较大。最后，&lt;strong&gt;验证误差和训练误差会收敛，但不会靠近&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;接下来，我们看一个具体的例子：&lt;/p&gt;
&lt;p&gt;样本包含两类数据，如下所示：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;data-visualization.png&#34; data-caption=&#34;Scatter Plot of data&#34;&gt;
&lt;img data-src=&#34;data-visualization.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Scatter Plot of data
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;针对该数据，我们拟合以下三个模型，看看它们的学习曲线如何：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;逻辑回归模型&lt;/li&gt;
&lt;li&gt;决策树模型&lt;/li&gt;
&lt;li&gt;支持向量机模型&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC

# Load data
df = pd.read_csv(&#39;data.csv&#39;)
X = np.array(df[[&#39;x1&#39;, &#39;x2&#39;]])
y = np.array(df[&#39;y&#39;])

# Fix random seed
np.random.seed(55)

def randomize(X, Y):
    permutation = np.random.permutation(Y.shape[0])
    X2 = X[permutation,:]
    Y2 = Y[permutation]
    return X2, Y2

X2, y2 = randomize(X, y)

# Construct three models to draw learning curves
estimators = {}

# Logistic Regression
estimators[&#39;LR&#39;] = LogisticRegression()

# Decision Tree
estimators[&#39;DT&#39;] = GradientBoostingClassifier()

# Support Vector Machine
estimators[&#39;SVC&#39;] = SVC(kernel=&#39;rbf&#39;, gamma=1000)

plt.figure(figsize=(12, 6))

for i, m in enumerate(estimators):
    train_sizes, train_scores, test_scores = learning_curve(
        estimators[m], X2, y2, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10))
    train_scores_mean = np.mean(train_scores, axis=1)
    # train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    # test_scores_std = np.std(test_scores, axis=1)

    plt.subplot(1, 3, i + 1)
    plt.grid()

    plt.title(&amp;quot;Learning Curves of {}&amp;quot;.format(m))
    plt.xlabel(&amp;quot;Training examples&amp;quot;)
    plt.ylabel(&amp;quot;Score&amp;quot;)

    plt.plot(train_scores_mean, &#39;o-&#39;, color=&amp;quot;g&amp;quot;,
             label=&amp;quot;Training score&amp;quot;)
    plt.plot(test_scores_mean, &#39;o-&#39;, color=&amp;quot;y&amp;quot;,
             label=&amp;quot;Cross-validation score&amp;quot;)


    plt.legend(loc=&amp;quot;best&amp;quot;)

plt.tight_layout()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以上代码将生成三个模型的学习曲线，我们来看一看：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;learning-curves.png&#34; &gt;
&lt;img data-src=&#34;learning-curves.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;根据这些曲线可以得出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;逻辑回归模型的训练和测试得分很低。&lt;/li&gt;
&lt;li&gt;决策树模型的训练和测试得分很高。&lt;/li&gt;
&lt;li&gt;支持向量机模型的训练得分很高，测试得分很低&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，决策树模型刚刚好，逻辑回归模型欠拟合，而支持向量机模型过拟合。&lt;/p&gt;
&lt;p&gt;同样，我们可以反转这些曲线（因为度量使用的是得分而不是误差）然后与下图进行对比。&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;learning-curves-2.png&#34; &gt;
&lt;img data-src=&#34;learning-curves-2.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;最后，我们看一下在实际情况中是否也这样，为此我们绘制模型的边界曲线：&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;models-boundary.png&#34; &gt;
&lt;img data-src=&#34;models-boundary.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;我们可以看到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;逻辑回归模型使用一条直线，这太简单了。在训练集上的效果不太好，因此欠拟合。&lt;/li&gt;
&lt;li&gt;决策树模型使用一个方形，拟合的很好，并能够泛化。因此，该模型效果很好。&lt;/li&gt;
&lt;li&gt;支持向量机模型实际上在每个点周围都画了一个小圆圈。它实际上是在记住训练集，无法泛化。因此，过拟合。&lt;/li&gt;
&lt;/ul&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;这里的测试实为验证，不同于最终的测试，后者指模型评估，前者为模型验证。 &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>逻辑回归（Logistic Regression）</title>
      <link>https://keris.github.io/zh/post/logistic-regression/</link>
      <pubDate>Tue, 14 Jan 2020 16:47:22 +0800</pubDate>
      <guid>https://keris.github.io/zh/post/logistic-regression/</guid>
      <description>&lt;h2&gt;目录&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#术语&#34;&gt;术语&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#logistic-function&#34;&gt;Logistic function&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#odds&#34;&gt;Odds&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#logit-function&#34;&gt;Logit function&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#逻辑回归&#34;&gt;逻辑回归&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#代价函数求导&#34;&gt;代价函数求导&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;术语&#34;&gt;术语&lt;/h2&gt;
&lt;p&gt;逻辑回归涉及到以下术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Logistic function&lt;/li&gt;
&lt;li&gt;Odds&lt;/li&gt;
&lt;li&gt;Logit&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;logistic-function&#34;&gt;Logistic function&lt;/h3&gt;
&lt;p&gt;逻辑回归中的 &lt;em&gt;Logistic&lt;/em&gt; 正是出于 &lt;em&gt;Logistic function&lt;/em&gt; ，它是一种 &lt;em&gt;sigmoid function&lt;/em&gt; ，其接受任意实值，输出一个0到1之间的值。
&lt;em&gt;标准的&lt;/em&gt; logistic funtion 定义如下：&lt;/p&gt;
&lt;p&gt;$$\sigma(z) = \frac{e^z}{e^z + 1} = \frac{1}{1 + e^{-z}}$$&lt;/p&gt;
&lt;p&gt;如下是它在区间$[-6, 6]$之间的图像：&lt;/p&gt;
&lt;p&gt;&lt;a title=&#34;logistic function curve&#34; href=&#34;https://commons.wikimedia.org/wiki/File:Logistic-curve.svg&#34;&gt;&lt;img width=&#34;512&#34; alt=&#34;Logistic-curve&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/512px-Logistic-curve.svg.png&#34;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在逻辑回归中，我们使用对数几率（log odds），&lt;strong&gt;并假定它是输入特征的线性组合&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;$$z = \ln \frac{p(x)}{1 - p(x)}= \theta_0 + \theta_1 x_1 + \theta_2 x_2 = \theta^T \cdot x$$&lt;/p&gt;
&lt;p&gt;由上式我们可以得到，&lt;/p&gt;
&lt;p&gt;$$p(x) = \sigma(z) = \frac{1}{1 + e^{-\theta^T \cdot x}}$$&lt;/p&gt;
&lt;p&gt;在逻辑回归模型中，这里的 $p(x)$ 为因变量在成功情形下的概率，即 $p(y=1 \mid x)$。&lt;/p&gt;
&lt;h3 id=&#34;odds&#34;&gt;Odds&lt;/h3&gt;
&lt;p&gt;如果 $p$ 表示一个事件发生的概率，那么odds定义为&lt;/p&gt;
&lt;p&gt;$$\text{odds} = \frac{p}{1 - p}$$
也就是说，odds为发生的概率除以不发生的概率，亦可以说为成功的概率除以失败的概率。&lt;/p&gt;
&lt;h3 id=&#34;logit-function&#34;&gt;Logit function&lt;/h3&gt;
&lt;p&gt;Logit为Log odds, logit function 定义为 logistic function的逆，即 $g = \sigma^{-1}$。显而易见，我们有&lt;/p&gt;
&lt;p&gt;$$g(p(x)) = \sigma_{-1}(p(x)) = \text{logit}\,p(x) = \ln(\frac{p(x)}{1 - p(x)}) = \theta^T \cdot x$$&lt;/p&gt;
&lt;h2 id=&#34;逻辑回归&#34;&gt;逻辑回归&lt;/h2&gt;
&lt;p&gt;逻辑回归是一个重要的机器学习算法，其目标是基于给定的数据$x$输出随机变量$y$为0或1的概率。&lt;/p&gt;
&lt;p&gt;考虑由$\theta$参数化的线性模型，&lt;/p&gt;
&lt;p&gt;$$h_\theta(x) = \frac{1}{1 + e^{-\theta^T \cdot x}} = \text{Pr}(y = 1 \mid x;\theta)$$&lt;/p&gt;
&lt;p&gt;从而，$\text{Pr}(y=0 \mid x;\theta) = 1 - h_\theta(x)$。&lt;/p&gt;
&lt;p&gt;因为$y \in \{0, 1 \}$，我们有&lt;/p&gt;
&lt;p&gt;$$\text{Pr}(y \mid x;\theta) = h_\theta(x)^y (1 - h_\theta(x))^{1 - y}
$$&lt;/p&gt;
&lt;p&gt;似然函数为&lt;/p&gt;
&lt;p&gt;$$\begin{aligned} L(\theta \mid x) &amp;amp;= \Pr(Y\mid X;\theta) \\&lt;br&gt;
&amp;amp;= \prod_i \Pr(y^{(i)} \mid x^{(i)};\theta) \\&lt;br&gt;
&amp;amp;= \prod_i h_\theta(x^{(i)})^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \end{aligned}$$&lt;/p&gt;
&lt;p&gt;一般地，我们最大化对数似然函数，&lt;/p&gt;
&lt;p&gt;$$\log L(\theta \mid x) = \sum_{i=1}^{m}\log \Pr(y^{(i)} \mid x^{(i)};\theta)$$&lt;/p&gt;
&lt;p&gt;定义代价函数如下：&lt;/p&gt;
&lt;p&gt;$$J(\theta) = -\frac{1}{m} \log L(\theta \mid x) = -\frac{1}{m} \sum_i^m (y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})(1 - h_\theta(x^{(i)})))$$&lt;/p&gt;
&lt;p&gt;容易看到，我们最大化对数似然函数就是最小化代价函数 $J(\theta)$。在机器学习中，我们使用梯度下降最小化代价函数。&lt;/p&gt;
&lt;h3 id=&#34;代价函数求导&#34;&gt;代价函数求导&lt;/h3&gt;
&lt;p&gt;下面我们对代价函数$J(\theta)$对$\theta$进行求导，我们先考虑在一个样本上的代价函数&lt;/p&gt;
&lt;p&gt;$$J_1(\theta) = -y \log h_\theta(x) - (1 - y)(1 - h_\theta(x))$$&lt;/p&gt;
&lt;p&gt;现在对$J_1(\theta)$对$\theta$进行求导：&lt;/p&gt;
&lt;p&gt;注意到$\frac{d\sigma(z)}{dz} = \sigma(z) (1 - \sigma(z))$, 我们有&lt;/p&gt;
&lt;p&gt;$$\begin{aligned}\frac{\partial}{\partial \theta_j} J_1(\theta) &amp;amp;= -y \frac{1}{h_\theta(x)} h_\theta(x) (1 - h_\theta(x)) x_j - (1 - y) \frac{1}{1 - h_\theta(x)} (-1) h_\theta(x) (1 - h_\theta(x)) x_j \\ &amp;amp;= (h_\theta(x) - y) x_j\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;有了以上结果，我们有&lt;/p&gt;
&lt;p&gt;$$\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m}\sum_i^m [h_\theta(x^{(i)}) - y^{(i)}]x_j^{(i)}$$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
